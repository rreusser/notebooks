<!doctype html>
<notebook theme="air">
  <title>Strange Attractors in WebGPU</title>
  <script id="intro" type="text/markdown">
# Strange Attractors in WebGPU

*This notebook is a 2026 WebGPU update of my 2021 post, [Strange Attractors on the GPU, Part 1: Implementation](https://observablehq.com/@rreusser/strange-attractors-on-the-gpu-part-1).*

This notebook walks through simulating a strange attractor on the GPU and then rendering particle tracks as lines. The essential feature of this notebook is that it accomplishes both particle updates and rendering to the screen without data ever touching the CPU.
  </script>
  <script id="webgpu-setup" type="module">
// Check for WebGPU support
if (!navigator.gpu) {
  display(html`<p style="color: red;">WebGPU is not supported in this browser.</p>`);
  throw new Error('WebGPU not supported');
}

const adapter = await navigator.gpu.requestAdapter();
if (!adapter) {
  throw new Error('Failed to get WebGPU adapter');
}

const device = await adapter.requestDevice();
const canvasFormat = navigator.gpu.getPreferredCanvasFormat();

invalidation.then(() => device.destroy());
  </script>
  <script id="main-canvas" type="module">
import { createElementStack } from './lib/element-stack.js'
import { expandable } from './lib/expandable.js'

const dpr = window.devicePixelRatio || 1;
const canvasWidth = Math.min(800, width);
const canvasHeight = Math.max(400, canvasWidth * 0.6);

const stack = createElementStack({
  width: canvasWidth,
  height: canvasHeight,
  layers: [{
    id: 'canvas',
    element: ({ current, width, height }) => {
      const canvas = current || document.createElement('canvas');
      canvas.id = 'attractor-canvas';
      canvas.width = Math.floor(width * dpr);
      canvas.height = Math.floor(height * dpr);
      canvas.style.width = `${width}px`;
      canvas.style.height = `${height}px`;
      return canvas;
    }
  }, {
    id: 'svg',
    element: ({ current, width, height }) =>
      (current ? d3.select(current) : d3.create("svg"))
        .attr("width", width)
        .attr("height", height)
        .style("cursor", "grab")
        .node()
  }]
});

const canvas = stack.elements.canvas;

const gpuContext = canvas.getContext('webgpu');
gpuContext.configure({
  device,
  format: canvasFormat,
  alphaMode: 'premultiplied'
});

// Create depth texture for proper z-ordering
const depthTexture = device.createTexture({
  label: 'depth-texture',
  size: [canvas.width, canvas.height],
  format: 'depth24plus',
  usage: GPUTextureUsage.RENDER_ATTACHMENT
});

const renderState = { dirty: true, depthTexture };

const figure = html`<figure style="margin: 0;" id="main-figure">
  ${stack.element}
</figure>`;

display(expandable(figure, {
  width: canvasWidth,
  height: canvasHeight,
  controls: '.attractor-controls',
  onResize(el, w, h) {
    stack.resize(w, h);
    canvas.width = Math.floor(w * dpr);
    canvas.height = Math.floor(h * dpr);
    // Recreate depth texture at new size
    renderState.depthTexture.destroy();
    renderState.depthTexture = device.createTexture({
      label: 'depth-texture',
      size: [canvas.width, canvas.height],
      format: 'depth24plus',
      usage: GPUTextureUsage.RENDER_ATTACHMENT
    });
    renderState.dirty = true;
    stack.dispatchEvent(new CustomEvent('update'));
  }
}));
  </script>
  <script id="controls" type="module">
const controlsContainer = html`<div class="attractor-controls"></div>`;

function ctrl(input) {
  controlsContainer.appendChild(input);
  return Generators.input(input);
}

const restartInput = Inputs.button('Restart');
const restart = ctrl(restartInput);

const simulateInput = Inputs.toggle({ label: 'Simulate', value: true });
const simulate = ctrl(simulateInput);

const particleCountInput = Inputs.range([1, 4096], {
  value: 20,
  label: 'Particle count',
  step: 1
});
const particleCount = ctrl(particleCountInput);

const stepCountInput = Inputs.range([1, 1024], {
  label: 'Track length',
  value: 100,
  transform: Math.log,
  step: 1
});
const stepCount = ctrl(stepCountInput);

const dtInput = Inputs.range([0.001, 0.1], {
  value: 0.02,
  label: 'Time step'
});
const dt = ctrl(dtInput);

const lineWidthInput = Inputs.range([1, 20], {
  value: 5,
  label: 'Line width',
  step: 0.5
});
const lineWidth = ctrl(lineWidthInput);

display(controlsContainer);
  </script>
  <script id="explanation-attractor" type="text/markdown">
### The Attractor

A strange attractor is a set of states toward which a dynamical system evolves over time. The [Lorenz System](https://en.wikipedia.org/wiki/Lorenz_system) is the canonical example. The particular attractor we're simulating here is the *Bouali Attractor*, described by Safieddine Bouali in [A 3D Strange Attractor with a Distinctive Silhouette. The Butterfly Effect Revisited](https://arxiv.org/abs/1311.6128). It is defined by the system of ordinary differential equations:

${tex.block`\begin{aligned}
\frac{dx}{dt} &= \alpha x(1 - y) - \beta z \\[0.5em]
\frac{dy}{dt} &= -\gamma y(1 - x^2) \\[0.5em]
\frac{dz}{dt} &= \mu x
\end{aligned}`}

with parameters ${tex`\alpha = 3`}, ${tex`\beta = 2.2`}, ${tex`\gamma = 1`}, ${tex`\mu = 1.51`}. These equations exhibit chaotic behavior; nearby trajectories diverge exponentially but remain bounded within the attractor's basin.
  </script>
  <script id="explanation-overview" type="text/markdown">
## From WebGL to WebGPU

[The original WebGL version of this simulation](https://observablehq.com/@rreusser/strange-attractors-on-the-gpu-part-1) stored particle state in a floating-point texture. Each row represented a particle's history as a ring buffer. The simulation was designed around the limitations of WebGL, in particular the lack of compute shaders and inability to read and write to the same texture (WebGL 2's [transform feedback](https://webgl2fundamentals.org/webgl/lessons/webgl-gpgpu.html#now-lets-do-it-with-transform-feedback) notwithstanding).

A naive WebGPU port would preserve this texture-based approach, replacing fragment shader hacks with proper compute shaders. That would require **three GPU operations per frame**:

1. **Integrate**: Read from the state texture, write new positions to a temporary texture
2. **Copy**: Transfer the temporary column back into the main state texture
3. **Draw**: Render lines by sampling the state texture in a vertex shader

WebGPU offers a better primitive for this workload which eliminates the copy step entirely: storage buffers with `read_write` access. This is an easy fit for the embarrassingly parallel task of integrating independent particles with no mutual interaction.

With storage buffers, each frame requires just two GPU operations:

1. **Integrate** the differential equation, reading the current state and writing one new state value for each particle.
2. **Draw** all line segments, joins, and caps using instanced triangle strip geometry.

For line rendering, we use the (WIP) [webgpu-lines](../webgpu-lines/) module. Lines are difficult to render well. The built-in line primitive in graphics APIs is typically limited to single-pixel width. To get smooth, variable-width lines with proper joins and caps, we build geometry from triangles in the vertex shader. The module handles this complexity, requiring only that we provide a function to compute vertex positions from our data.
  </script>
  <script id="explanation-state" type="text/markdown">
## State Layout

The state of our ordinary differential equation (ODE) is represented by the three-component vector ${tex`(x, y, z)`}. We store these in a flat storage buffer as `vec3f` elements. The ${tex`j^{th}`} time step of the ${tex`i^{th}`} particle is represented by the vector:

${tex.block`\mathbf{p}_j^{(i)} = (x_j^{(i)}, y_j^{(i)}, z_j^{(i)})`}

We use **particle-major ordering**: all time steps for particle 0 come first, then all time steps for particle 1, and so on. The buffer index for a given particle and step is `particle * stepCount + step`.

As we step the ODE, we compute one new history point for each particle track. To avoid shifting the entire history on every iteration, we treat each particle's slice as a **ring buffer**. At each time step ${tex`j`}, we use the previous position, ${tex`p_{j-1}^{(i)}`}, to compute the next, ${tex`p_j^{(i)}`}. When we reach the end of the slice, we loop back to the start, overwriting the oldest time step with the newest.
  </script>
  <script id="state-buffer" type="module">
particleCount; stepCount; restart;

// Create storage buffer for state
// Each element stores (x, y, z) as vec3f for one particle at one timestep
// Layout: buffer[particleId * stepCount + stepIndex] = position
// Using storage buffer with read_write access eliminates the need for a temp buffer
// Note: array<vec3f> has 16-byte stride due to WGSL alignment rules
const stateBuffer = device.createBuffer({
  label: 'state-buffer',
  size: particleCount * stepCount * 16, // vec3f with 16-byte stride per position
  usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST
});

// Track current column for ring buffer
const simState = { currentColumn: 0, t: 0 };

invalidation.then(() => {
  stateBuffer.destroy();
});
  </script>
  <script id="explanation-init" type="text/markdown">
## Computation

Unlike fragment shaders which operate on pixels being rasterized, **compute shaders** dispatch a grid of threads which can read from and write to arbitrary locations in GPU resources.

### Initialization

For historical reasons which I didn't see it necessary to change, we initialize particles with a compute shader, although we could write data from the CPU to a buffer just fine.

We start by initializing particle positions within a sphere centered near the attractor. Generating good pseudorandom numbers on a GPU is tricky, so we use a low-discrepancy quasirandom number generator described by Martin Roberts in *[The Unreasonable Effectiveness of Quasirandom Sequences](http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/)*.
  </script>
  <script id="init-shader" type="module">
// Compute shader to initialize particle state
const initShaderCode = /* wgsl */`
@group(0) @binding(0) var<storage, read_write> stateBuffer: array<vec3f>;

struct Uniforms {
  origin: vec3f,
  scale: f32,
  stepCount: u32,
  particleCount: u32,
}
@group(0) @binding(1) var<uniform> uniforms: Uniforms;

// Quasirandom sequence: http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/
fn quasirandom(n: f32) -> vec3f {
  let g = 1.22074408460575947536;
  return fract(0.5 + n * vec3f(1.0 / g, 1.0 / (g * g), 1.0 / (g * g * g))).zyx;
}

fn sphericalRandom(n: f32) -> vec3f {
  let rand = quasirandom(n);
  let u = rand.x * 2.0 - 1.0;
  let theta = 6.283185307179586 * rand.y;
  let r = sqrt(1.0 - u * u);
  return vec3f(r * cos(theta), r * sin(theta), u) * sqrt(rand.z);
}

@compute @workgroup_size(64)
fn main(@builtin(global_invocation_id) gid: vec3u) {
  let particle = gid.x;
  if (particle >= uniforms.particleCount) { return; }

  // Initialize all columns for this particle with the same position
  let pos = uniforms.origin + uniforms.scale * sphericalRandom(f32(particle) + 0.5);
  let baseIdx = particle * uniforms.stepCount;
  for (var col = 0u; col < uniforms.stepCount; col++) {
    stateBuffer[baseIdx + col] = pos;
  }
}
`;

const initShaderModule = device.createShaderModule({
  label: 'init-shader',
  code: initShaderCode
});

const initPipeline = device.createComputePipeline({
  label: 'init-pipeline',
  layout: 'auto',
  compute: {
    module: initShaderModule,
    entryPoint: 'main'
  }
});

const initUniformBuffer = device.createBuffer({
  label: 'init-uniforms',
  size: 32, // vec3f origin (12) + f32 scale (4) + u32 stepCount (4) + u32 particleCount (4) + padding
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const initBindGroup = device.createBindGroup({
  label: 'init-bind-group',
  layout: initPipeline.getBindGroupLayout(0),
  entries: [
    { binding: 0, resource: { buffer: stateBuffer } },
    { binding: 1, resource: { buffer: initUniformBuffer } }
  ]
});

function initializeState() {
  // Update uniforms
  const uniformData = new ArrayBuffer(32);
  const f32 = new Float32Array(uniformData);
  const u32 = new Uint32Array(uniformData);
  f32[0] = 0;  // origin.x
  f32[1] = 1;  // origin.y (center near y=1 for this attractor)
  f32[2] = 0;  // origin.z
  f32[3] = 1;  // scale
  u32[4] = stepCount;
  u32[5] = particleCount;
  device.queue.writeBuffer(initUniformBuffer, 0, uniformData);

  const encoder = device.createCommandEncoder();
  const pass = encoder.beginComputePass();
  pass.setPipeline(initPipeline);
  pass.setBindGroup(0, initBindGroup);
  pass.dispatchWorkgroups(Math.ceil(particleCount / 64));
  pass.end();
  device.queue.submit([encoder.finish()]);

  simState.currentColumn = 0;
  simState.t = 0;
  renderState.dirty = true;
}

// Initialize on load and restart
initializeState();

invalidation.then(() => {
  initUniformBuffer.destroy();
});
  </script>
  <script id="explanation-integration" type="text/markdown">
### Integration

To step the ODE, we use the [fourth-order Runge-Kutta](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#The_Runge%E2%80%93Kutta_method) (RK4) method. The integration shader reads from index `particle * stepCount + srcColumn` and writes to `particle * stepCount + dstColumn`. Since these are different indices (no particle reads what another particle writes), there are no data races.
  </script>
  <script id="attractor-wgsl" type="module">
// The strange attractor equations
const attractorWGSL = /* wgsl */`
fn derivative(x: f32, y: f32, z: f32, t: f32) -> vec3f {
  let alpha = 3.0;
  let beta = 2.20;
  let gamma = 1.0;
  let mu = 1.510;
  return vec3f(
    alpha * x * (1.0 - y) - beta * z,
    -gamma * y * (1.0 - x * x),
    mu * x
  );
}
`;
  </script>
  <script id="integrate-shader" type="module">
// Integration shader: reads and writes to a single storage buffer
const integrateShaderCode = /* wgsl */`
@group(0) @binding(0) var<storage, read_write> stateBuffer: array<vec3f>;

struct Uniforms {
  dt: f32,
  t: f32,
  srcColumn: u32,
  dstColumn: u32,
  stepCount: u32,
  particleCount: u32,
}
@group(0) @binding(1) var<uniform> uniforms: Uniforms;

${attractorWGSL}

fn deriv(p: vec3f, t: f32) -> vec3f {
  return derivative(p.x, p.y, p.z, t);
}

@compute @workgroup_size(64)
fn main(@builtin(global_invocation_id) gid: vec3u) {
  let particle = gid.x;
  if (particle >= uniforms.particleCount) { return; }

  // Read current state from source column
  let readIdx = particle * uniforms.stepCount + uniforms.srcColumn;
  let p = stateBuffer[readIdx];

  // RK4 integration
  let dt = uniforms.dt;
  let t = uniforms.t;
  let k1 = deriv(p, t);
  let k2 = deriv(p + 0.5 * dt * k1, t + 0.5 * dt);
  let k3 = deriv(p + 0.5 * dt * k2, t + 0.5 * dt);
  let k4 = deriv(p + dt * k3, t + dt);

  var newP = p + (dt / 6.0) * (k1 + k4 + 2.0 * (k2 + k3));

  // If particle diverges, reset near origin
  if (dot(newP, newP) > 1e6) {
    newP = newP * 0.0001;
  }

  // Write to destination column
  let writeIdx = particle * uniforms.stepCount + uniforms.dstColumn;
  stateBuffer[writeIdx] = newP;
}
`;

const integrateShaderModule = device.createShaderModule({
  label: 'integrate-shader',
  code: integrateShaderCode
});

const integratePipeline = device.createComputePipeline({
  label: 'integrate-pipeline',
  layout: 'auto',
  compute: {
    module: integrateShaderModule,
    entryPoint: 'main'
  }
});

const integrateUniformBuffer = device.createBuffer({
  label: 'integrate-uniforms',
  size: 32, // f32 dt + f32 t + u32 srcColumn + u32 dstColumn + u32 stepCount + u32 particleCount + padding
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const integrateBindGroup = device.createBindGroup({
  label: 'integrate-bind-group',
  layout: integratePipeline.getBindGroupLayout(0),
  entries: [
    { binding: 0, resource: { buffer: stateBuffer } },
    { binding: 1, resource: { buffer: integrateUniformBuffer } }
  ]
});

invalidation.then(() => {
  integrateUniformBuffer.destroy();
});
  </script>
  <script id="explanation-rendering" type="text/markdown">
## Line Rendering

In the timeless words of Matt DesLauriers, *[Drawing Lines is Hard](https://mattdesl.svbtle.com/drawing-lines-is-hard)*. Browsers limit the built-in line primitive to a single pixel width, so for any reasonably well-rendered lines, we need to build our own geometry.

The (WIP) [webgpu-lines](../webgpu-lines/) module renders lines as instanced triangle strips, with one instance per line segment. Each instance draws the segment itself plus half of the adjacent joins. A four-point sliding window (previous, start, end, next) provides the context needed to compute join geometry.

We don't pass vertex positions directly. Instead, we provide a vertex function which uses the integer vertex ID to read particle buffer data. To handle the ring buffer, we add an offset to the step index that shifts based on which column was most recently written. The oldest data is always at the "start" of the rendered line, and the newest at the end.

### Line Breaks

Multiple particle tracks are rendered in a single draw call. We separate them using sentinel values of `w = 0` in clip space. When the vertex function returns a position with `w = 0`, the line renderer interprets this as a break, skipping one instance and reworking adjacent joins into end caps.
  </script>
  <script id="camera-setup" type="module">
// Simple orbital camera using mouse drag
// phi = azimuthal angle (rotation around Y axis)
// theta = polar angle (elevation from horizontal plane, positive = looking down)
const cameraState = {
  phi: 0.8,
  theta: 0.3,
  distance: 7,
  center: [0, 2, 0],
  fov: Math.PI / 4,
  near: 0.01,
  far: 100
};

function getProjectionViewMatrix(aspectRatio) {
  const { phi, theta, distance, center, fov, near, far } = cameraState;

  // Camera position in spherical coordinates
  const x = center[0] + distance * Math.cos(theta) * Math.cos(phi);
  const y = center[1] + distance * Math.sin(theta);
  const z = center[2] + distance * Math.cos(theta) * Math.sin(phi);

  // View matrix (lookAt)
  const eye = [x, y, z];
  const forward = [center[0] - x, center[1] - y, center[2] - z];
  const len = Math.sqrt(forward[0]**2 + forward[1]**2 + forward[2]**2);
  forward[0] /= len; forward[1] /= len; forward[2] /= len;

  const up = [0, 1, 0];
  const right = [
    forward[1] * up[2] - forward[2] * up[1],
    forward[2] * up[0] - forward[0] * up[2],
    forward[0] * up[1] - forward[1] * up[0]
  ];
  const rlen = Math.sqrt(right[0]**2 + right[1]**2 + right[2]**2);
  right[0] /= rlen; right[1] /= rlen; right[2] /= rlen;

  const newUp = [
    right[1] * forward[2] - right[2] * forward[1],
    right[2] * forward[0] - right[0] * forward[2],
    right[0] * forward[1] - right[1] * forward[0]
  ];

  // View matrix
  const view = new Float32Array([
    right[0], newUp[0], -forward[0], 0,
    right[1], newUp[1], -forward[1], 0,
    right[2], newUp[2], -forward[2], 0,
    -(right[0]*eye[0] + right[1]*eye[1] + right[2]*eye[2]),
    -(newUp[0]*eye[0] + newUp[1]*eye[1] + newUp[2]*eye[2]),
    (forward[0]*eye[0] + forward[1]*eye[1] + forward[2]*eye[2]),
    1
  ]);

  // Projection matrix (perspective)
  const f = 1.0 / Math.tan(fov / 2);
  const rangeInv = 1 / (near - far);
  const proj = new Float32Array([
    f / aspectRatio, 0, 0, 0,
    0, f, 0, 0,
    0, 0, (near + far) * rangeInv, -1,
    0, 0, near * far * rangeInv * 2, 0
  ]);

  // Multiply proj * view
  const result = new Float32Array(16);
  for (let i = 0; i < 4; i++) {
    for (let j = 0; j < 4; j++) {
      let sum = 0;
      for (let k = 0; k < 4; k++) {
        sum += proj[i + k*4] * view[k + j*4];
      }
      result[i + j*4] = sum;
    }
  }

  return result;
}

// Wire up mouse/touch drag for camera rotation
const svg = d3.select(stack.elements.svg);
let isDragging = false;
let lastX = 0, lastY = 0;

function onMouseMove(event) {
  if (!isDragging) return;
  const dx = event.clientX - lastX;
  const dy = event.clientY - lastY;
  cameraState.phi += dx * 0.01;
  cameraState.theta = Math.max(-Math.PI/2 + 0.01, Math.min(Math.PI/2 - 0.01, cameraState.theta + dy * 0.01));
  lastX = event.clientX;
  lastY = event.clientY;
  renderState.dirty = true;
}

function onMouseUp() {
  if (!isDragging) return;
  isDragging = false;
  svg.style('cursor', 'grab');
  window.removeEventListener('mousemove', onMouseMove);
  window.removeEventListener('mouseup', onMouseUp);
}

svg.on('mousedown', (event) => {
  event.preventDefault();
  isDragging = true;
  lastX = event.clientX;
  lastY = event.clientY;
  svg.style('cursor', 'grabbing');
  window.addEventListener('mousemove', onMouseMove);
  window.addEventListener('mouseup', onMouseUp);
});

svg.on('wheel', (event) => {
  event.preventDefault();
  cameraState.distance *= 1 + event.deltaY * 0.001;
  cameraState.distance = Math.max(1, Math.min(50, cameraState.distance));
  renderState.dirty = true;
});

// Touch support: single finger to rotate, pinch to zoom
let lastTouchDist = 0;

svg.on('touchstart', (event) => {
  event.preventDefault();
  if (event.touches.length === 1) {
    isDragging = true;
    lastX = event.touches[0].clientX;
    lastY = event.touches[0].clientY;
  } else if (event.touches.length === 2) {
    // Pinch-to-zoom: record initial distance between fingers
    const dx = event.touches[1].clientX - event.touches[0].clientX;
    const dy = event.touches[1].clientY - event.touches[0].clientY;
    lastTouchDist = Math.sqrt(dx * dx + dy * dy);
  }
});

svg.on('touchmove', (event) => {
  event.preventDefault();
  if (event.touches.length === 1 && isDragging) {
    const dx = event.touches[0].clientX - lastX;
    const dy = event.touches[0].clientY - lastY;
    cameraState.phi += dx * 0.01;
    cameraState.theta = Math.max(-Math.PI/2 + 0.01, Math.min(Math.PI/2 - 0.01, cameraState.theta + dy * 0.01));
    lastX = event.touches[0].clientX;
    lastY = event.touches[0].clientY;
    renderState.dirty = true;
  } else if (event.touches.length === 2) {
    // Pinch-to-zoom
    const dx = event.touches[1].clientX - event.touches[0].clientX;
    const dy = event.touches[1].clientY - event.touches[0].clientY;
    const dist = Math.sqrt(dx * dx + dy * dy);
    if (lastTouchDist > 0) {
      const scale = lastTouchDist / dist;
      cameraState.distance *= scale;
      cameraState.distance = Math.max(1, Math.min(50, cameraState.distance));
      renderState.dirty = true;
    }
    lastTouchDist = dist;
  }
});

svg.on('touchend', (event) => {
  isDragging = false;
  lastTouchDist = 0;
});
  </script>
  <script id="line-rendering" type="module">
import { createGPULines } from '../webgpu-lines/webgpu-lines.js';

// Vertex shader that samples the state buffer
const vertexShaderBody = /* wgsl */`
@group(1) @binding(0) var<storage, read> stateBuffer: array<vec3f>;
@group(1) @binding(1) var<uniform> projViewMatrix: mat4x4f;

struct LineUniforms {
  columnOffset: u32,
  stepCount: u32,
  particleCount: u32,
}
@group(1) @binding(2) var<uniform> lineUniforms: LineUniforms;

struct Vertex {
  position: vec4f,
  width: f32,
  particleId: f32,
}

fn getVertex(index: u32) -> Vertex {
  // Decode buffer index from vertex index
  // Index layout: for each particle, stepCount points, then a break
  let pointsPerParticle = lineUniforms.stepCount + 1u; // +1 for line break
  let particle = index / pointsPerParticle;
  let step = index % pointsPerParticle;

  // Check if this is a line break point
  if (step >= lineUniforms.stepCount) {
    return Vertex(vec4f(0.0, 0.0, 0.0, 0.0), 0.0, f32(particle));
  }

  // Compute buffer index with ring buffer offset
  let stepIndex = (step + lineUniforms.columnOffset) % lineUniforms.stepCount;
  let bufferIdx = particle * lineUniforms.stepCount + stepIndex;

  // Sample position from state buffer
  let pos = stateBuffer[bufferIdx];

  // Project to clip space
  let projected = projViewMatrix * vec4f(pos, 1.0);

  // Use particle ID for slight z offset to reduce z-fighting
  var p = projected;
  let w = p.w;
  p = p / w;
  p.z = p.z - 0.0002 * abs(f32(particle) / f32(lineUniforms.particleCount) - 0.5);
  p = p * w;

  return Vertex(p, uniforms.width, f32(particle) / f32(lineUniforms.particleCount));
}
`;

const fragmentShaderBody = /* wgsl */`
fn getColor(lineCoord: vec2f, particleId: f32) -> vec4f {
  let color1 = vec3f(0.55, 0.89, 0.65);
  let color2 = vec3f(0.11, 0.32, 0.65);
  let baseColor = mix(color1, color2, particleId);

  // 1-pixel border: lineCoord is in [-1, 1], so 1 pixel = 2/width in normalized coords
  let borderWidth = 2.0 / uniforms.width;
  let dist = length(lineCoord.xy);
  let borderColor = vec3f(1.0);
  let color = mix(baseColor, borderColor, smoothstep(1.0 - 2.0 * borderWidth, 1.0 - borderWidth, dist));

  return vec4f(color, 1.0);
}
`;

const gpuLines = createGPULines(device, {
  format: canvasFormat,
  depthFormat: 'depth24plus',
  join: 'bevel',
  cap: 'round',
  capResolution: 4,
  vertexShaderBody,
  fragmentShaderBody,
});

// Create buffers for line rendering
const projViewBuffer = device.createBuffer({
  label: 'proj-view-matrix',
  size: 64,
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const lineUniformBuffer = device.createBuffer({
  label: 'line-uniforms',
  size: 16, // columnOffset, stepCount, particleCount + padding
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const lineBindGroup = device.createBindGroup({
  layout: gpuLines.getBindGroupLayout(1),
  entries: [
    { binding: 0, resource: { buffer: stateBuffer } },
    { binding: 1, resource: { buffer: projViewBuffer } },
    { binding: 2, resource: { buffer: lineUniformBuffer } }
  ]
});

invalidation.then(() => {
  gpuLines.destroy();
  projViewBuffer.destroy();
  lineUniformBuffer.destroy();
});
  </script>
  <script id="explanation-draw-loop" type="text/markdown">
## Draw Loop

Finally, we put it all together into a frame loop. For each frame:

1. dispatch the integrate compute shader to compute new particle positions
2. Update the buffer offset uniform so lines render from oldest to newest
3. Draw lines by calling into the webgpu-lines module
  </script>
  <script id="render-loop" type="module">
import { createFrameLoop } from './lib/frame-loop.js';

lineWidth; simulate; dt;

const loop = createFrameLoop(() => {
  // Simulation step
  if (simulate) {
    // Calculate source and destination columns
    const srcColumn = simState.currentColumn;
    const dstColumn = (simState.currentColumn + 1) % stepCount;

    // Update integration uniforms (dt, t, srcColumn, dstColumn, stepCount, particleCount)
    const integrateData = new ArrayBuffer(32);
    const f32 = new Float32Array(integrateData);
    const u32 = new Uint32Array(integrateData);
    f32[0] = dt;
    f32[1] = simState.t;
    u32[2] = srcColumn;
    u32[3] = dstColumn;
    u32[4] = stepCount;
    u32[5] = particleCount;
    device.queue.writeBuffer(integrateUniformBuffer, 0, integrateData);

    // Run integration (single pass reads and writes to storage buffer)
    const encoder = device.createCommandEncoder();
    const integratePass = encoder.beginComputePass();
    integratePass.setPipeline(integratePipeline);
    integratePass.setBindGroup(0, integrateBindGroup);
    integratePass.dispatchWorkgroups(Math.ceil(particleCount / 64));
    integratePass.end();
    device.queue.submit([encoder.finish()]);

    // Advance time and column
    simState.t += dt;
    simState.currentColumn = dstColumn;
    renderState.dirty = true;
  }

  // Render
  if (renderState.dirty) {
    const aspectRatio = canvas.width / canvas.height;
    const projView = getProjectionViewMatrix(aspectRatio);
    device.queue.writeBuffer(projViewBuffer, 0, projView);

    // Update line uniforms (columnOffset, stepCount, particleCount as u32)
    const lineData = new ArrayBuffer(16);
    const u32 = new Uint32Array(lineData);
    // columnOffset: the oldest column, so lines render from oldest to newest
    u32[0] = (simState.currentColumn + 1) % stepCount;
    u32[1] = stepCount;
    u32[2] = particleCount;
    device.queue.writeBuffer(lineUniformBuffer, 0, lineData);

    const encoder = device.createCommandEncoder();

    const pass = encoder.beginRenderPass({
      colorAttachments: [{
        view: gpuContext.getCurrentTexture().createView(),
        loadOp: 'clear',
        storeOp: 'store',
        clearValue: { r: 1.0, g: 1.0, b: 1.0, a: 1.0 }
      }],
      depthStencilAttachment: {
        view: renderState.depthTexture.createView(),
        depthClearValue: 1.0,
        depthLoadOp: 'clear',
        depthStoreOp: 'store'
      }
    });

    // Total vertex count: (stepCount + 1) per particle for line breaks
    const totalVertexCount = particleCount * (stepCount + 1);

    gpuLines.draw(pass, {
      vertexCount: totalVertexCount,
      width: lineWidth * dpr,
      resolution: [canvas.width, canvas.height]
    }, [lineBindGroup]);

    pass.end();
    device.queue.submit([encoder.finish()]);

    if (!simulate) {
      renderState.dirty = false;
    }
  }
});

invalidation.then(() => loop.cancel());
  </script>
  <script id="restart-handler" type="module">
// Handle restart button
restart;
initializeState();
  </script>
  <script id="conclusion" type="text/markdown">
## Summary

I'm thrilled that my new WebGPU line rendering module worked out so well. It fills a big gap in what I need WebGPU for, and I'm eager to keep using it.

The switch from hacky WebGL workarounds to proper WebGPU primitives turned out well. I haven't done the full timing analysis I should have, but we went from three passes (integrate, copy, render) to just two (integrate, render).

And of course, the techniques here generalize beyond our strange attractors test case. Any particle system where you want to visualize trails—fluid simulations, n-body systems, gradient flows—can use this same ring buffer approach for efficient GPU-based track rendering. 
  </script>
</notebook>
