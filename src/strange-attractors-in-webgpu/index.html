<!doctype html>
<notebook theme="air">
  <title>Strange Attractors in WebGPU</title>
  <script id="intro" type="text/markdown">
# Strange Attractors in WebGPU

*This notebook is a 2026 WebGPU update of my 2021 post, [Strange Attractors on the GPU, Part 1: Implementation](https://observablehq.com/@rreusser/strange-attractors-on-the-gpu-part-1).*

This notebook walks through simulating a strange attractor on the GPU and then rendering particle tracks as lines. The essential feature of this notebook is that it accomplishes both particle updates and rendering to the screen without data ever touching the CPU.
  </script>
  <script id="webgpu-setup" type="module">
import { createWebGPUContext } from './lib/webgpu-canvas.js';

const { device, canvasFormat } = await createWebGPUContext();

invalidation.then(() => device.destroy());
  </script>
  <script id="main-canvas" type="module">
import { createElementStack } from './lib/element-stack.js'
import { expandable } from './lib/expandable.js'

const dpr = window.devicePixelRatio || 1;
const canvasWidth = Math.min(800, width);
const canvasHeight = Math.max(400, canvasWidth * 0.6);

const stack = createElementStack({
  width: canvasWidth,
  height: canvasHeight,
  layers: [{
    id: 'canvas',
    element: ({ current, width, height }) => {
      const canvas = current || document.createElement('canvas');
      canvas.id = 'attractor-canvas';
      canvas.width = Math.floor(width * dpr);
      canvas.height = Math.floor(height * dpr);
      canvas.style.width = `${width}px`;
      canvas.style.height = `${height}px`;
      return canvas;
    }
  }, {
    id: 'svg',
    element: ({ current, width, height }) =>
      (current ? d3.select(current) : d3.create("svg"))
        .attr("width", width)
        .attr("height", height)
        .style("cursor", "grab")
        .node()
  }]
});

const canvas = stack.elements.canvas;

const gpuContext = canvas.getContext('webgpu');
gpuContext.configure({
  device,
  format: canvasFormat,
  alphaMode: 'premultiplied'
});

// Create MSAA textures for anti-aliasing
const sampleCount = 4;

const msaaColorTexture = device.createTexture({
  label: 'msaa-color-texture',
  size: [canvas.width, canvas.height],
  format: canvasFormat,
  sampleCount,
  usage: GPUTextureUsage.RENDER_ATTACHMENT
});

const depthTexture = device.createTexture({
  label: 'depth-texture',
  size: [canvas.width, canvas.height],
  format: 'depth24plus',
  sampleCount,
  usage: GPUTextureUsage.RENDER_ATTACHMENT
});

const renderState = { dirty: true, depthTexture, msaaColorTexture, sampleCount };

const figure = html`<figure style="margin: 0;" id="main-figure">
  ${stack.element}
</figure>`;

display(expandable(figure, {
  width: canvasWidth,
  height: canvasHeight,
  controls: '.attractor-controls',
  onResize(el, w, h) {
    stack.resize(w, h);
    canvas.width = Math.floor(w * dpr);
    canvas.height = Math.floor(h * dpr);
    // Recreate MSAA textures at new size
    renderState.msaaColorTexture.destroy();
    renderState.msaaColorTexture = device.createTexture({
      label: 'msaa-color-texture',
      size: [canvas.width, canvas.height],
      format: canvasFormat,
      sampleCount: renderState.sampleCount,
      usage: GPUTextureUsage.RENDER_ATTACHMENT
    });
    renderState.depthTexture.destroy();
    renderState.depthTexture = device.createTexture({
      label: 'depth-texture',
      size: [canvas.width, canvas.height],
      format: 'depth24plus',
      sampleCount: renderState.sampleCount,
      usage: GPUTextureUsage.RENDER_ATTACHMENT
    });
    renderState.dirty = true;
    stack.dispatchEvent(new CustomEvent('update'));
  }
}));
  </script>
  <script id="controls" type="module">
const controlsContainer = html`<div class="attractor-controls"></div>`;

function ctrl(input) {
  controlsContainer.appendChild(input);
  return Generators.input(input);
}

const restartInput = Inputs.button('Restart');
const restart = ctrl(restartInput);

const simulateInput = Inputs.toggle({ label: 'Simulate', value: true });
const simulate = ctrl(simulateInput);

const particleCountInput = Inputs.range([1, 4096], {
  value: 200,
  label: 'Particle count',
  step: 1
});
const particleCount = ctrl(particleCountInput);

const stepCountInput = Inputs.range([1, 1024], {
  label: 'Track length',
  value: 50,
  transform: Math.log,
  step: 1
});
const stepCount = ctrl(stepCountInput);

const dtInput = Inputs.range([0.001, 0.1], {
  value: 0.02,
  label: 'Time step'
});
const dt = ctrl(dtInput);

const lineWidthInput = Inputs.range([1, 20], {
  value: 6,
  label: 'Line width',
  step: 0.5
});
const lineWidth = ctrl(lineWidthInput);

display(controlsContainer);
  </script>
  <script id="explanation-attractor" type="text/markdown">
### The Attractor

A strange attractor is a set of states toward which a dynamical system evolves over time. The [Lorenz System](https://en.wikipedia.org/wiki/Lorenz_system) is the canonical example. The particular attractor we're simulating here is the *Bouali Attractor*, described by Safieddine Bouali in [A 3D Strange Attractor with a Distinctive Silhouette. The Butterfly Effect Revisited](https://arxiv.org/abs/1311.6128). It is defined by the system of ordinary differential equations:

${tex.block`\begin{aligned}
\frac{dx}{dt} &= \alpha x(1 - y) - \beta z \\[0.5em]
\frac{dy}{dt} &= -\gamma y(1 - x^2) \\[0.5em]
\frac{dz}{dt} &= \mu x
\end{aligned}`}

with parameters ${tex`\alpha = 3`}, ${tex`\beta = 2.2`}, ${tex`\gamma = 1`}, ${tex`\mu = 1.51`}. These equations exhibit chaotic behavior; nearby trajectories diverge exponentially but remain bounded within the attractor's basin.
  </script>
  <script id="explanation-overview" type="text/markdown">
## From WebGL to WebGPU

[The original WebGL version of this simulation](https://observablehq.com/@rreusser/strange-attractors-on-the-gpu-part-1) stored particle state in a floating-point texture. Each row represented a particle's history as a ring buffer. The simulation was designed around the limitations of WebGL, in particular the lack of compute shaders and inability to read and write to the same texture (WebGL 2's [transform feedback](https://webgl2fundamentals.org/webgl/lessons/webgl-gpgpu.html#now-lets-do-it-with-transform-feedback) notwithstanding).

A naive WebGPU port would preserve this texture-based approach, replacing fragment shader hacks with proper compute shaders. That would require **three GPU operations per frame**:

1. **Integrate**: Read from the state texture, write new positions to a temporary texture
2. **Copy**: Transfer the temporary column back into the main state texture
3. **Draw**: Render lines by sampling the state texture in a vertex shader

I tried this texture-based approach, but when I didn't see much performance improvement over the buffer approach, I switched back to storage buffers with `read_write` access. This is a natural fit for the embarrassingly parallel task of integrating independent particles with no mutual interaction, and it eliminates the copy step entirely.

With storage buffers, each frame requires just two GPU operations:

1. **Integrate** the differential equation, reading the current state and writing one new state value for each particle.
2. **Draw** all line segments, joins, and caps using instanced triangle strip geometry.

For line rendering, we use the [webgpu-instanced-lines](https://github.com/rreusser/webgpu-instanced-lines) module. Lines are difficult to render well. The built-in line primitive in graphics APIs is typically limited to single-pixel width. To get smooth, variable-width lines with proper joins and caps, we build geometry from triangles in the vertex shader. The module handles this complexity, requiring only that we provide a function to compute vertex positions from our data.

  </script>
  <script id="perf-note" type="text/markdown">
**A note on performance**: For reasons I don't understand, this WebGPU implementation performs slightly worse than the [original WebGL 1 version](https://observablehq.com/@rreusser/strange-attractors-on-the-gpu-part-1) despite using almost identically the same logic and line rendering implementation. It's relatively unclear to me why that is, though I take it as a lesson that new technology can unlock new techniques, algorithms, and capabilities, but just blindly throwing it at the stuff you're already doing isn't necessarily going to lead to magic improvements.
  </script>
  <script id="explanation-state" type="text/markdown">
## State Layout

The state of our ordinary differential equation (ODE) is represented by the three-component vector ${tex`(x, y, z)`}. We store these in a flat storage buffer as `vec3f` elements. The ${tex`j^{th}`} time step of the ${tex`i^{th}`} particle is represented by the vector:

${tex.block`\mathbf{p}_j^{(i)} = (x_j^{(i)}, y_j^{(i)}, z_j^{(i)})`}

We use **particle-major ordering**: all time steps for particle 0 come first, then all time steps for particle 1, and so on. The buffer index for a given particle and step is `particle * stepCount + step`.

As we step the ODE, we compute one new history point for each particle track. To avoid shifting the entire history on every iteration, we treat each particle's slice as a **ring buffer**. At each time step ${tex`j`}, we use the previous position, ${tex`p_{j-1}^{(i)}`}, to compute the next, ${tex`p_j^{(i)}`}. When we reach the end of the slice, we loop back to the start, overwriting the oldest time step with the newest.
  </script>
  <script id="state-buffer" type="module">
particleCount; stepCount; restart;

// Create storage buffer for particle state
// Layout: particle-major ordering, each entry is vec4f (16 bytes)
// Index: particle * stepCount + step
const stateBuffer = device.createBuffer({
  label: 'state-buffer',
  size: particleCount * stepCount * 16, // vec4f per point
  usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST
});

// Track current step for ring buffer
const simState = { currentStep: 0, t: 0 };

invalidation.then(() => {
  stateBuffer.destroy();
});
  </script>
  <script id="explanation-init" type="text/markdown">
## Computation

Unlike fragment shaders which operate on pixels being rasterized, **compute shaders** dispatch a grid of threads which can read from and write to arbitrary locations in GPU resources.

### Initialization

For historical reasons which I didn't see it necessary to change, we initialize particles with a compute shader, although we could write data from the CPU to a buffer just fine.

We start by initializing particle positions within a sphere centered near the attractor. Generating good pseudorandom numbers on a GPU is tricky, so we use a low-discrepancy quasirandom number generator described by Martin Roberts in *[The Unreasonable Effectiveness of Quasirandom Sequences](http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/)*.
  </script>
  <script id="init-shader" type="module">
// Compute shader to initialize particle state (writes to buffer)
const initShaderCode = /* wgsl */`
@group(0) @binding(0) var<storage, read_write> state: array<vec4f>;

struct Uniforms {
  origin: vec3f,
  scale: f32,
  stepCount: u32,
  particleCount: u32,
}
@group(0) @binding(1) var<uniform> uniforms: Uniforms;

// Quasirandom sequence: http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/
fn quasirandom(n: f32) -> vec3f {
  let g = 1.22074408460575947536;
  return fract(0.5 + n * vec3f(1.0 / g, 1.0 / (g * g), 1.0 / (g * g * g))).zyx;
}

fn sphericalRandom(n: f32) -> vec3f {
  let rand = quasirandom(n);
  let u = rand.x * 2.0 - 1.0;
  let theta = 6.283185307179586 * rand.y;
  let r = sqrt(1.0 - u * u);
  return vec3f(r * cos(theta), r * sin(theta), u) * sqrt(rand.z);
}

@compute @workgroup_size(64)
fn main(@builtin(global_invocation_id) gid: vec3u) {
  let particle = gid.x;
  if (particle >= uniforms.particleCount) { return; }

  // Initialize all steps for this particle with the same position
  let pos = uniforms.origin + uniforms.scale * sphericalRandom(f32(particle) + 0.5);
  for (var step = 0u; step < uniforms.stepCount; step++) {
    // Buffer index: particle * stepCount + step
    let idx = particle * uniforms.stepCount + step;
    state[idx] = vec4f(pos, 1.0);
  }
}
`;

const initShaderModule = device.createShaderModule({
  label: 'init-shader',
  code: initShaderCode
});

const initPipeline = device.createComputePipeline({
  label: 'init-pipeline',
  layout: 'auto',
  compute: {
    module: initShaderModule,
    entryPoint: 'main'
  }
});

const initUniformBuffer = device.createBuffer({
  label: 'init-uniforms',
  size: 32, // vec3f origin (12) + f32 scale (4) + u32 stepCount (4) + u32 particleCount (4) + padding
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const initBindGroup = device.createBindGroup({
  label: 'init-bind-group',
  layout: initPipeline.getBindGroupLayout(0),
  entries: [
    { binding: 0, resource: { buffer: stateBuffer } },
    { binding: 1, resource: { buffer: initUniformBuffer } }
  ]
});

function initializeState() {
  // Update uniforms
  const uniformData = new ArrayBuffer(32);
  const f32 = new Float32Array(uniformData);
  const u32 = new Uint32Array(uniformData);
  f32[0] = 0;  // origin.x
  f32[1] = 1;  // origin.y (center near y=1 for this attractor)
  f32[2] = 0;  // origin.z
  f32[3] = 1;  // scale
  u32[4] = stepCount;
  u32[5] = particleCount;
  device.queue.writeBuffer(initUniformBuffer, 0, uniformData);

  const encoder = device.createCommandEncoder();
  const pass = encoder.beginComputePass();
  pass.setPipeline(initPipeline);
  pass.setBindGroup(0, initBindGroup);
  pass.dispatchWorkgroups(Math.ceil(particleCount / 64));
  pass.end();
  device.queue.submit([encoder.finish()]);

  simState.currentStep = 0;
  simState.t = 0;
  renderState.dirty = true;
}

// Initialize on load and restart
initializeState();

invalidation.then(() => {
  initUniformBuffer.destroy();
});
  </script>
  <script id="explanation-integration" type="text/markdown">
### Integration

To step the ODE, we use the [fourth-order Runge-Kutta](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#The_Runge%E2%80%93Kutta_method) (RK4) method. The integration shader reads from index `particle * stepCount + srcColumn` and writes to `particle * stepCount + dstColumn`. Since these are different indices (no particle reads what another particle writes), there are no data races.
  </script>
  <script id="attractor-wgsl" type="module">
// The strange attractor equations
const attractorWGSL = /* wgsl */`
fn derivative(x: f32, y: f32, z: f32, t: f32) -> vec3f {
  let alpha = 3.0;
  let beta = 2.20;
  let gamma = 1.0;
  let mu = 1.510;
  return vec3f(
    alpha * x * (1.0 - y) - beta * z,
    -gamma * y * (1.0 - x * x),
    mu * x
  );
}
`;
  </script>
  <script id="integrate-shader" type="module">
// Integration shader: reads from and writes to storage buffer directly
// With read_write access, no ping-pong needed - just read src step and write dst step
const integrateShaderCode = /* wgsl */`
@group(0) @binding(0) var<storage, read_write> state: array<vec4f>;

struct Uniforms {
  dt: f32,
  t: f32,
  srcStep: u32,
  dstStep: u32,
  stepCount: u32,
  particleCount: u32,
}
@group(0) @binding(1) var<uniform> uniforms: Uniforms;

${attractorWGSL}

fn deriv(p: vec3f, t: f32) -> vec3f {
  return derivative(p.x, p.y, p.z, t);
}

@compute @workgroup_size(64)
fn main(@builtin(global_invocation_id) gid: vec3u) {
  let particle = gid.x;
  if (particle >= uniforms.particleCount) { return; }

  // Read current state from source step
  let srcIdx = particle * uniforms.stepCount + uniforms.srcStep;
  let p = state[srcIdx].xyz;

  // RK4 integration
  let dt = uniforms.dt;
  let t = uniforms.t;
  let k1 = deriv(p, t);
  let k2 = deriv(p + 0.5 * dt * k1, t + 0.5 * dt);
  let k3 = deriv(p + 0.5 * dt * k2, t + 0.5 * dt);
  let k4 = deriv(p + dt * k3, t + dt);

  var newP = p + (dt / 6.0) * (k1 + k4 + 2.0 * (k2 + k3));

  // If particle diverges, reset near origin
  if (dot(newP, newP) > 1e6) {
    newP = newP * 0.0001;
  }

  // Write to destination step
  let dstIdx = particle * uniforms.stepCount + uniforms.dstStep;
  state[dstIdx] = vec4f(newP, 1.0);
}
`;

const integrateShaderModule = device.createShaderModule({
  label: 'integrate-shader',
  code: integrateShaderCode
});

const integratePipeline = device.createComputePipeline({
  label: 'integrate-pipeline',
  layout: 'auto',
  compute: {
    module: integrateShaderModule,
    entryPoint: 'main'
  }
});

const integrateUniformBuffer = device.createBuffer({
  label: 'integrate-uniforms',
  size: 32, // f32 dt + f32 t + u32 srcStep + u32 dstStep + u32 stepCount + u32 particleCount + padding
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const integrateBindGroup = device.createBindGroup({
  label: 'integrate-bind-group',
  layout: integratePipeline.getBindGroupLayout(0),
  entries: [
    { binding: 0, resource: { buffer: stateBuffer } },
    { binding: 1, resource: { buffer: integrateUniformBuffer } }
  ]
});

invalidation.then(() => {
  integrateUniformBuffer.destroy();
});
  </script>
  <script id="explanation-rendering" type="text/markdown">
## Line Rendering

In the timeless words of Matt DesLauriers, *[Drawing Lines is Hard](https://mattdesl.svbtle.com/drawing-lines-is-hard)*. Browsers limit the built-in line primitive to a single pixel width, so for any reasonably well-rendered lines, we need to build our own geometry.

The [webgpu-instanced-lines](https://github.com/rreusser/webgpu-instanced-lines) module renders lines as instanced triangle strips, with one instance per line segment. Each instance draws the segment itself plus half of the adjacent joins. A four-point sliding window (previous, start, end, next) provides the context needed to compute join geometry.

We don't pass vertex positions directly. Instead, we provide a vertex function which uses the integer vertex ID to read particle buffer data. To handle the ring buffer, we add an offset to the step index that shifts based on which column was most recently written. The oldest data is always at the "start" of the rendered line, and the newest at the end.
  </script>

  <script id="line-breaks" type="text/markdown">
### Line Breaks

Multiple particle tracks are rendered in a single draw call. We separate them using sentinel values of `w = 0` in clip space. When the vertex function returns a position with `w = 0`, the line renderer interprets this as a break, skipping one instance and reworking adjacent joins into end caps.

### Code

A selection of the code is presented below. We import [webgpu-instanced-lines](https://www.npmjs.com/package/webgpu-instanced-lines) from NPM and instantiate a line renderer. The main call and vertex shader are printed below, but for a comprehensive, straightforward example with all the pieces in place, see [this example](https://rreusser.github.io/webgpu-instanced-lines/lorenz.html) from the module.
  </script>
  <script id="gpu-lines-init" type="module" pinned>
import { createGPULines } from 'npm:webgpu-instanced-lines';

const gpuLines = createGPULines(device, {
  colorTargets: [{
    format: canvasFormat,
    blend: {
      color: {
        srcFactor: 'src-alpha',
        dstFactor: 'one-minus-src-alpha',
        operation: 'add'
      },
      alpha: {
        srcFactor: 'one',
        dstFactor: 'one-minus-src-alpha',
        operation: 'add'
      }
    }
  }],
  depthStencil: { format: 'depth24plus', depthWriteEnabled: true, depthCompare: 'less' },
  multisample: { count: renderState.sampleCount, alphaToCoverageEnabled: false },
  join: 'bevel',
  cap: 'square',
  vertexShaderBody,
  fragmentShaderBody,
});

invalidation.then(() => gpuLines.destroy());
  </script>

  <script id="camera-setup" type="module">
// Simple orbital camera using mouse drag
// phi = azimuthal angle (rotation around Y axis)
// theta = polar angle (elevation from horizontal plane, positive = looking down)
const cameraState = {
  phi: 0.8,
  theta: 0.3,
  distance: 7,
  center: [0, 2, 0],
  fov: Math.PI / 4,
  near: 0.01,
  far: 100
};

// Pre-allocate matrices to avoid per-frame allocations
const _view = new Float32Array(16);
const _proj = new Float32Array(16);
const _projView = new Float32Array(16);

function getProjectionViewMatrix(aspectRatio) {
  const { phi, theta, distance, center, fov, near, far } = cameraState;

  // Camera position in spherical coordinates
  const x = center[0] + distance * Math.cos(theta) * Math.cos(phi);
  const y = center[1] + distance * Math.sin(theta);
  const z = center[2] + distance * Math.cos(theta) * Math.sin(phi);

  // View matrix (lookAt)
  const eyeX = x, eyeY = y, eyeZ = z;
  let fwdX = center[0] - x, fwdY = center[1] - y, fwdZ = center[2] - z;
  const len = Math.sqrt(fwdX*fwdX + fwdY*fwdY + fwdZ*fwdZ);
  fwdX /= len; fwdY /= len; fwdZ /= len;

  // right = forward × up (up = 0,1,0)
  let rightX = fwdY * 0 - fwdZ * 1;
  let rightY = fwdZ * 0 - fwdX * 0;
  let rightZ = fwdX * 1 - fwdY * 0;
  const rlen = Math.sqrt(rightX*rightX + rightY*rightY + rightZ*rightZ);
  rightX /= rlen; rightY /= rlen; rightZ /= rlen;

  // newUp = right × forward
  const upX = rightY * fwdZ - rightZ * fwdY;
  const upY = rightZ * fwdX - rightX * fwdZ;
  const upZ = rightX * fwdY - rightY * fwdX;

  // View matrix (reuse pre-allocated array)
  _view[0] = rightX; _view[1] = upX; _view[2] = -fwdX; _view[3] = 0;
  _view[4] = rightY; _view[5] = upY; _view[6] = -fwdY; _view[7] = 0;
  _view[8] = rightZ; _view[9] = upZ; _view[10] = -fwdZ; _view[11] = 0;
  _view[12] = -(rightX*eyeX + rightY*eyeY + rightZ*eyeZ);
  _view[13] = -(upX*eyeX + upY*eyeY + upZ*eyeZ);
  _view[14] = (fwdX*eyeX + fwdY*eyeY + fwdZ*eyeZ);
  _view[15] = 1;

  // Projection matrix (perspective, reuse pre-allocated array)
  const f = 1.0 / Math.tan(fov / 2);
  const rangeInv = 1 / (near - far);
  _proj[0] = f / aspectRatio; _proj[1] = 0; _proj[2] = 0; _proj[3] = 0;
  _proj[4] = 0; _proj[5] = f; _proj[6] = 0; _proj[7] = 0;
  _proj[8] = 0; _proj[9] = 0; _proj[10] = (near + far) * rangeInv; _proj[11] = -1;
  _proj[12] = 0; _proj[13] = 0; _proj[14] = near * far * rangeInv * 2; _proj[15] = 0;

  // Multiply proj * view (reuse pre-allocated result)
  for (let i = 0; i < 4; i++) {
    for (let j = 0; j < 4; j++) {
      let sum = 0;
      for (let k = 0; k < 4; k++) {
        sum += _proj[i + k*4] * _view[k + j*4];
      }
      _projView[i + j*4] = sum;
    }
  }

  return _projView;
}

// Wire up mouse/touch drag for camera rotation
const svg = d3.select(stack.elements.svg);
let isDragging = false;
let lastX = 0, lastY = 0;

function onMouseMove(event) {
  if (!isDragging) return;
  const dx = event.clientX - lastX;
  const dy = event.clientY - lastY;
  cameraState.phi += dx * 0.01;
  cameraState.theta = Math.max(-Math.PI/2 + 0.01, Math.min(Math.PI/2 - 0.01, cameraState.theta + dy * 0.01));
  lastX = event.clientX;
  lastY = event.clientY;
  renderState.dirty = true;
}

function onMouseUp() {
  if (!isDragging) return;
  isDragging = false;
  svg.style('cursor', 'grab');
  window.removeEventListener('mousemove', onMouseMove);
  window.removeEventListener('mouseup', onMouseUp);
}

svg.on('mousedown', (event) => {
  event.preventDefault();
  isDragging = true;
  lastX = event.clientX;
  lastY = event.clientY;
  svg.style('cursor', 'grabbing');
  window.addEventListener('mousemove', onMouseMove);
  window.addEventListener('mouseup', onMouseUp);
});

svg.on('wheel', (event) => {
  event.preventDefault();
  cameraState.distance *= 1 + event.deltaY * 0.001;
  cameraState.distance = Math.max(1, Math.min(50, cameraState.distance));
  renderState.dirty = true;
});

// Touch support: single finger to rotate, pinch to zoom
let lastTouchDist = 0;

svg.on('touchstart', (event) => {
  event.preventDefault();
  if (event.touches.length === 1) {
    isDragging = true;
    lastX = event.touches[0].clientX;
    lastY = event.touches[0].clientY;
  } else if (event.touches.length === 2) {
    // Pinch-to-zoom: record initial distance between fingers
    const dx = event.touches[1].clientX - event.touches[0].clientX;
    const dy = event.touches[1].clientY - event.touches[0].clientY;
    lastTouchDist = Math.sqrt(dx * dx + dy * dy);
  }
});

svg.on('touchmove', (event) => {
  event.preventDefault();
  if (event.touches.length === 1 && isDragging) {
    const dx = event.touches[0].clientX - lastX;
    const dy = event.touches[0].clientY - lastY;
    cameraState.phi += dx * 0.01;
    cameraState.theta = Math.max(-Math.PI/2 + 0.01, Math.min(Math.PI/2 - 0.01, cameraState.theta + dy * 0.01));
    lastX = event.touches[0].clientX;
    lastY = event.touches[0].clientY;
    renderState.dirty = true;
  } else if (event.touches.length === 2) {
    // Pinch-to-zoom
    const dx = event.touches[1].clientX - event.touches[0].clientX;
    const dy = event.touches[1].clientY - event.touches[0].clientY;
    const dist = Math.sqrt(dx * dx + dy * dy);
    if (lastTouchDist > 0) {
      const scale = lastTouchDist / dist;
      cameraState.distance *= scale;
      cameraState.distance = Math.max(1, Math.min(50, cameraState.distance));
      renderState.dirty = true;
    }
    lastTouchDist = dist;
  }
});

svg.on('touchend', (event) => {
  isDragging = false;
  lastTouchDist = 0;
});
  </script>
  <script id="line-rendering" type="module">
// Vertex shader that loads from the state buffer
const vertexShaderBody = /* wgsl */`// vertexShaderBody:

@group(1) @binding(0) var<storage, read> state: array<vec4f>;
@group(1) @binding(1) var<uniform> projViewMatrix: mat4x4f;

struct LineUniforms {
  stepOffset: u32,  // Ring buffer offset
  stepCount: u32,
  particleCount: u32,
  width: f32,
}
@group(1) @binding(2) var<uniform> lineUniforms: LineUniforms;

struct Vertex {
  position: vec4f,
  width: f32,
  t: f32,
  velocity: f32,
  lineWidth: f32,
}

// Bouali attractor derivative for velocity computation
fn attractorDerivative(pos: vec3f) -> vec3f {
  let alpha = 3.0;
  let beta = 2.2;
  let gamma = 1.0;
  let mu = 1.51;
  return vec3f(
    alpha * pos.x * (1.0 - pos.y) - beta * pos.z,
    -gamma * pos.y * (1.0 - pos.x * pos.x),
    mu * pos.x
  );
}

fn getVertex(index: u32) -> Vertex {
  // Decode buffer index from vertex index
  let pointsPerParticle = lineUniforms.stepCount + 1u; // +1 for line break
  let particle = index / pointsPerParticle;
  let step = index % pointsPerParticle;

  // Check if this is a line break point
  if (step >= lineUniforms.stepCount) {
    return Vertex(vec4f(0), 0.0, 0.0, 0.0, 0.0);
  }

  // Compute buffer index with ring buffer offset (modular arithmetic for wrap-around)
  let bufferStep = (step + lineUniforms.stepOffset) % lineUniforms.stepCount;
  let bufferIdx = particle * lineUniforms.stepCount + bufferStep;

  // Load position from state buffer
  let pos = state[bufferIdx].xyz;

  // Compute velocity from attractor derivative
  let speed = length(attractorDerivative(pos));
  let normalizedVelocity = clamp(speed / 10.0, 0.0, 1.0);

  // Progress along the track (0 = oldest, 1 = newest)
  let t = f32(step) / f32(lineUniforms.stepCount - 1u);

  // Project to clip space
  let projected = projViewMatrix * vec4f(pos, 1.0);

  // Line width tapers from thin (old) to thick (new)
  let lineWidth = lineUniforms.width * (0.3 + 0.7 * t);

  return Vertex(projected, lineWidth, t, normalizedVelocity, lineWidth);
}
`;

const fragmentShaderBody = /* wgsl */`
// Rainbow color palette from webgpu-instanced-lines lorenz example
fn rainbow(p: vec2f) -> vec3f {
  let theta = p.x * 6.283185;
  let c = cos(theta);
  let s = sin(theta);
  let m1 = mat3x3f(
    0.5230851,  0.56637411, 0.46725319,
    0.12769652, 0.14082407, 0.13691271,
   -0.25934743,-0.12121582, 0.2348705
  );
  let m2 = mat3x3f(
    0.3555664, -0.11472876,-0.01250831,
    0.15243126,-0.03668075, 0.0765231,
   -0.00192128,-0.01350681,-0.0036526
  );
  return m1 * vec3f(1.0, p.y * 2.0 - 1.0, s) +
         m2 * vec3f(c, s * c, c * c - s * s);
}

fn getColor(lineCoord: vec2f, t: f32, velocity: f32, lineWidth: f32) -> vec4f {
  let sdf = length(lineCoord) * lineWidth;
  let isCap = abs(lineCoord.x) > 0.0;

  // Rainbow color based on velocity, with saturation from track progress
  var color = rainbow(vec2f(velocity, t));

  if (isCap && dot(lineCoord, lineCoord) > 1.0) { discard; }

  // Dark border effect
  let borderWidth = 4.0;
  let borderMask = smoothstep(lineWidth - borderWidth - 0.75, lineWidth - borderWidth + 0.75, sdf);
  color = mix(color, vec3f(0.0), borderMask * 0.8);

  return vec4f(color, 1.0);
}
`;
  </script>
  <script id="line-buffers" type="module">
// Create buffers for line rendering
const projViewBuffer = device.createBuffer({
  label: 'proj-view-matrix',
  size: 64,
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const lineUniformBuffer = device.createBuffer({
  label: 'line-uniforms',
  size: 16, // stepOffset (u32), stepCount (u32), particleCount (u32), width (f32)
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const lineBindGroup = device.createBindGroup({
  layout: gpuLines.getBindGroupLayout(1),
  entries: [
    { binding: 0, resource: { buffer: stateBuffer } },
    { binding: 1, resource: { buffer: projViewBuffer } },
    { binding: 2, resource: { buffer: lineUniformBuffer } }
  ]
});

invalidation.then(() => {
  projViewBuffer.destroy();
  lineUniformBuffer.destroy();
});
  </script>
  <script id="vertex-shader-code" type="module">
display(html`<pre><code class="language-wgsl">${vertexShaderBody}</code></pre>`);
  </script>
  <script id="explanation-draw-loop" type="text/markdown">
## Draw Loop

Finally, we put it all together into a frame loop. For each frame:

1. dispatch the integrate compute shader to compute new particle positions
2. Update the buffer offset uniform so lines render from oldest to newest
3. Draw lines by calling into the webgpu-lines module
  </script>
  <script id="render-loop" type="module">
import { createFrameLoop } from './lib/frame-loop.js';

lineWidth; simulate; dt;

// Pre-allocate uniform data buffers to avoid per-frame allocations
const _integrateData = new ArrayBuffer(32);
const _integrateF32 = new Float32Array(_integrateData);
const _integrateU32 = new Uint32Array(_integrateData);

const _lineData = new ArrayBuffer(16);
const _lineU32 = new Uint32Array(_lineData);
const _lineF32 = new Float32Array(_lineData);

const loop = createFrameLoop(() => {
  // Use a single command encoder for both compute and render to avoid
  // synchronization barriers between separate submissions
  const encoder = device.createCommandEncoder();
  let needsSubmit = false;

  // Simulation step
  if (simulate) {
    // Calculate source and destination steps
    const srcStep = simState.currentStep;
    const dstStep = (simState.currentStep + 1) % stepCount;

    // Update integration uniforms (dt, t, srcStep, dstStep, stepCount, particleCount)
    _integrateF32[0] = dt;
    _integrateF32[1] = simState.t;
    _integrateU32[2] = srcStep;
    _integrateU32[3] = dstStep;
    _integrateU32[4] = stepCount;
    _integrateU32[5] = particleCount;
    device.queue.writeBuffer(integrateUniformBuffer, 0, _integrateData);

    // Run integration: reads from and writes to stateBuffer directly
    const integratePass = encoder.beginComputePass();
    integratePass.setPipeline(integratePipeline);
    integratePass.setBindGroup(0, integrateBindGroup);
    integratePass.dispatchWorkgroups(Math.ceil(particleCount / 64));
    integratePass.end();

    // Advance time and step (no texture copy needed with buffer approach)
    simState.t += dt;
    simState.currentStep = dstStep;
    renderState.dirty = true;
    needsSubmit = true;
  }

  // Render
  if (renderState.dirty) {
    const aspectRatio = canvas.width / canvas.height;
    const projView = getProjectionViewMatrix(aspectRatio);
    device.queue.writeBuffer(projViewBuffer, 0, projView);

    // Update line uniforms (stepOffset, stepCount, particleCount, width)
    // stepOffset: the oldest step index, so lines render from oldest to newest
    // Since we just wrote to currentStep, oldest is (currentStep + 1) % stepCount
    _lineU32[0] = (simState.currentStep + 1) % stepCount;
    _lineU32[1] = stepCount;
    _lineU32[2] = particleCount;
    _lineF32[3] = lineWidth * dpr;
    device.queue.writeBuffer(lineUniformBuffer, 0, _lineData);

    // Total vertex count: (stepCount + 1) per particle for line breaks
    const totalVertexCount = particleCount * (stepCount + 1);

    // Update gpuLines uniforms BEFORE beginning render pass to avoid mid-pass writes
    const lineDrawProps = {
      vertexCount: totalVertexCount,
      width: lineWidth * dpr,
      resolution: [canvas.width, canvas.height]
    };
    gpuLines.updateUniforms(lineDrawProps);

    const pass = encoder.beginRenderPass({
      colorAttachments: [{
        view: renderState.msaaColorTexture.createView(),
        resolveTarget: gpuContext.getCurrentTexture().createView(),
        loadOp: 'clear',
        storeOp: 'store',
        clearValue: { r: 1.0, g: 1.0, b: 1.0, a: 1.0 }
      }],
      depthStencilAttachment: {
        view: renderState.depthTexture.createView(),
        depthClearValue: 1.0,
        depthLoadOp: 'clear',
        depthStoreOp: 'store'
      }
    });

    gpuLines.draw(pass, { ...lineDrawProps, skipUniformUpdate: true }, [lineBindGroup]);

    pass.end();
    needsSubmit = true;

    if (!simulate) {
      renderState.dirty = false;
    }
  }

  // Single submission for all GPU work this frame
  if (needsSubmit) {
    device.queue.submit([encoder.finish()]);
  }
});

invalidation.then(() => loop.cancel());
  </script>
  <script id="restart-handler" type="module">
// Handle restart button
restart;
initializeState();
  </script>
  <script id="conclusion" type="text/markdown">
## Summary

I'm thrilled that my new WebGPU line rendering module worked out so well. It fills a big gap in what I need WebGPU for, and I'm eager to keep using it.

The switch from hacky WebGL workarounds to proper WebGPU primitives turned out well. I haven't done the full timing analysis I should have, but we went from three passes (integrate, copy, render) to just two (integrate, render).

And of course, the techniques here generalize beyond our strange attractors test case. Any particle system where you want to visualize trails—fluid simulations, n-body systems, gradient flows—can use this same ring buffer approach for efficient GPU-based track rendering. 
  </script>
</notebook>
