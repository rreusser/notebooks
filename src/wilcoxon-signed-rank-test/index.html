<!doctype html>
<notebook theme="air">
  <title>Wilcoxon Signed-Rank Test for Performance Testing</title>
  <script id="39" type="text/markdown">
    # Wilcoxon Signed-Rank Test for Performance Testing
  </script>
  <script id="41" type="text/markdown">
    ## The Problem
  </script>
  <script id="68" type="text/markdown">
    Imagine you've modified an algorithm and want to know whether its performance has changed.
  </script>
  <script id="42" type="text/markdown">
    You run your test suite before and after the change, measuring execution time for each test case. It’s easy enough to come up with an *ad hoc* performance metric and get a sense of good/bad, but it’s tricky to be more rigorous, accounting for the different baseline performance of each test, separating signal from noise, and determining the statistical significance of the results.
  </script>
  <script id="44" type="text/markdown">
    Of the numerous applicable statistical tests, the [**Wilcoxon signed-rank test**](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test) achieves a nice balance in this scenario by focusing on the *ranks* of the changes rather than their *absolute values*.
  </script>
  <script id="78" type="text/markdown">
    First though, a brief disclaimer. I've never been very good with statistics and I’m not a great person to elucidate even a relatively basic concept like this. I became aware of the Wilcoxon signed-rank test at a previous job when it was used for this sort of benchmarking, but I was afraid of it. Like a mystical artifact I admire without understanding, I put it on a shelf somewhere in a dark closet in the back of my mind and decided to dust it off when the time came to finally use it. That time has arrived, so I’m using this notebook to rid myself of any fear and figure out what’s really going on.
  </script>
  <script id="45" type="text/markdown">
    ## When to Use This Test
  </script>
  <script id="95" type="text/markdown">
    The Wilcoxon signed-rank test might be a good fit if you’ve measured some indicator for performance of something and want to determine whether the performance has improved or degraded. This could be something like wall clock run time. The key thing for this test is that the measurements are paired, like a single test suite run before and after some change.
  </script>
  <script id="94" type="text/markdown">
    Measuring algorithm performance can be weird. Things are pretty consistent, except every now and then garbage collection or some other browser tab eats up all the CPU cycles and you get some extreme outliers. The Wilcoxon signed-rank test is non-parametric, meaning it doesn’t assume your measurements follow a particular distribution. Instead, it ranks the measurements by magnitude and uses the numerical rank as the input, thus preserving some information about the relative size of changes while making it less sensitive to noise and outliers.
  </script>
  <script id="80" type="text/markdown">
    There are, of course, tests for normally distributed data ([paired t-test](https://en.wikipedia.org/wiki/Student%27s_t-test)) or for when you know the sign of changes only ([sign test](https://en.wikipedia.org/wiki/Sign_test)) or when the tests are ranked but not paired ([Mann–Whitney U test](https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test)). In short, there are many methods for many configurations, but if I had my choice of setting up some basic benchmarking, the Wilcoxon signed-rank test would be a pretty good fit.
  </script>
  <script id="74" type="text/markdown">
    ## How the Test Works
  </script>
  <script id="73" type="text/markdown">
    Consider measuring the performance of an algorithm before and after some changes, say ${tex`t_{\text{before},i}`} and ${tex`t_{\text{after},i}.`} The Wilcoxon signed-rank test proceeds in several steps.
  </script>
  <script id="69" type="text/markdown">
    1. **Calculate differences**. For each test case ${tex`i`}, compute the difference

        ${tex.block`d_i = t_{\text{after},i} - t_{\text{before},i},`}

        where positive values indicate the algorithm got slower, negative values indicate it got faster.

  </script>
  <script id="70" type="text/markdown">
    2. **Rank the absolute differences**. Ignore the signs temporarily and rank the absolute differences ${tex`|d_i|`} from smallest (rank 1) to largest (rank ${tex`n`}). If there are ties, assign each tied value the average of the ranks they would have received.

  </script>
  <script id="71" type="text/markdown">
    3. **Apply signs to ranks**. Restore the signs from the original differences to the ranks. This gives us signed ranks ${tex`R_i`}.

  </script>
  <script id="72" type="text/markdown">
    4. **Sum ranks by sign**. Calculate:
        - ${tex`W^+ = \sum_{i: d_i > 0} R_i`} (sum of ranks where algorithm got slower)
        - ${tex`W^- = \sum_{i: d_i < 0} |R_i|`} (sum of absolute ranks where algorithm got faster)

        Note that ${tex`W^+ + W^- = \frac{n(n+1)}{2}`}, the sum of all ranks from 1 to ${tex`n`}.

  </script>
  <script id="7" type="text/markdown">
    5. **Test Statistic and Null Hypothesis**. The test statistic is ${tex`W = \min(W^+, W^-)`}. Under the null hypothesis that there's no systematic difference (the median difference is zero), we expect ${tex`W^+`} and ${tex`W^-`} to be roughly equal. A small value of ${tex`W`} indicates one-sided behavior.

        For large samples (${tex`n > 10`}), ${tex`W`} is approximately normally distributed with
        ${tex.block`\text{mean} = \mu_W = \frac{n(n+1)}{4}`}
        ${tex.block`\text{variance} = \sigma^2_W = \frac{n(n+1)(2n+1)}{24}`}

        We can compute a z-score,
        ${tex.block`z = \frac{W - \mu_W}{\sigma_W}`}
        and derive a p-value from the standard normal distribution.

       A [**continuity correction**](https://www.mathspanda.com/A2FM/Lessons/Normal_approximations_with_Wilcoxon_tests_LESSON.pdf) accounts for approximating a discrete distribution (integer ranks) with a continuous distribution (normal). Since ${tex`W`} can only take integer values, we adjust by 0.5 in the direction of the mean, which improves the accuracy of the p-value for small to moderate sample sizes.

       I sorta get the reasoning, but the precise reasoning is a bit fuzzy to me. At the end of the day, we use the adjusted equation,

        ${tex.block` z = \frac{W - \mu_W + \frac{1}{2} \text{sgn}(W - \mu_W)}{\sigma_W}.`}
  </script>
  <script id="90" type="text/markdown">
    6. **P-value**. Finally, we compute a [“two-tailed” p-value](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/) using the normal CDF function. After reading the above reference and working out a line or two of algebra, we end up with
        ${tex.block`p = 1 - \text{erf}(|z|).`}
  </script>
  <script id="3" type="text/x-typescript">
    // Generate synthetic performance data
    function generatePerformanceData(numTestCases, baselineTimeMin, baselineTimeMax,
                                     noiseStdDevFraction, truePerformanceChange, seed = 42) {

      // Simple seeded random number generator
      let rng = seed;
      const random = () => {
        rng = (rng * 1664525 + 1013904223) % 4294967296;
        return rng / 4294967296;
      };

      const boxMuller = () => {
        const u1 = random();
        const u2 = random();
        return Math.sqrt(-2 * Math.log(u1)) * Math.cos(2 * Math.PI * u2);
      };

      const testCases = [];

      for (let i = 0; i < numTestCases; i++) {
        // Each test case has its own baseline time
        const baselineTime = baselineTimeMin + random() * (baselineTimeMax - baselineTimeMin);
        const noiseStdDev = baselineTime * noiseStdDevFraction;

        // Performance before the change (baseline + noise)
        const timeBefore = baselineTime + boxMuller() * noiseStdDev;

        // Performance after the change (baseline * (1 + change) + noise)
        const timeAfter = baselineTime * (1 + truePerformanceChange) + boxMuller() * noiseStdDev;

        testCases.push({
          id: `test_${i + 1}`,
          baselineTime: baselineTime,
          timeBefore: Math.max(0.1, timeBefore),
          timeAfter: Math.max(0.1, timeAfter)
        });
      }

      return testCases;
    }
  </script>
  <script id="5" type="text/x-typescript">
    // Calculate the Wilcoxon signed-rank test
    function wilcoxonSignedRank(pairedData) {
      // Step 1: Calculate differences and their absolute values
      const differences = pairedData.map(d => ({
        diff: d.timeAfter - d.timeBefore,
        absDiff: Math.abs(d.timeAfter - d.timeBefore),
        id: d.id
      })).filter(d => d.diff !== 0); // Remove zeros

      if (differences.length === 0) {
        return {error: "All differences are zero"};
      }

      // Step 2: Rank the absolute differences
      const sorted = [...differences].sort((a, b) => a.absDiff - b.absDiff);

      // Assign ranks, handling ties by averaging
      const ranks = [];
      let i = 0;
      while (i < sorted.length) {
        let j = i;
        // Find all tied values
        while (j < sorted.length && sorted[j].absDiff === sorted[i].absDiff) {
          j++;
        }
        // Assign average rank to all tied values
        const avgRank = (i + 1 + j) / 2;
        for (let k = i; k < j; k++) {
          ranks.push({
            ...sorted[k],
            rank: avgRank
          });
        }
        i = j;
      }

      // Step 3 & 4: Sum ranks for positive and negative differences
      let wPlus = 0;  // Sum of ranks for positive differences (slower)
      let wMinus = 0; // Sum of ranks for negative differences (faster)

      ranks.forEach(r => {
        if (r.diff > 0) {
          wPlus += r.rank;
        } else {
          wMinus += r.rank;
        }
      });

      // Step 5: Test statistic is the smaller of the two
      const W = Math.min(wPlus, wMinus);
      const n = ranks.length;

      // Calculate z-score for normal approximation (valid for n > 10)
      const expectedW = n * (n + 1) / 4;
      const varianceW = n * (n + 1) * (2 * n + 1) / 24;
      const stdDevW = Math.sqrt(varianceW);

      // Continuity correction
      const zScore = (W - expectedW + 0.5 * Math.sign(W - expectedW)) / stdDevW;

      // Two-tailed p-value using normal approximation
      const pValue = 2 * (1 - normalCDF(Math.abs(zScore)));

      return {
        n,
        wPlus,
        wMinus,
        W,
        zScore,
        pValue,
        ranks,
        significant: pValue < 0.05
      };
    }

    // Standard normal cumulative distribution function
    function normalCDF(x) {
      const t = 1 / (1 + 0.2316419 * Math.abs(x));
      const d = 0.3989423 * Math.exp(-x * x / 2);
      const prob = d * t * (0.3193815 + t * (-0.3565638 + t * (1.781478 + t * (-1.821256 + t * 1.330274))));
      return x > 0 ? 1 - prob : prob;
    }
  </script>
  <script id="8" type="text/markdown">
    ## Interactive Demo

    Adjust the parameters below to explore how different scenarios affect the results:
  </script>
  <script id="16" type="module">
    const numTestCases = view(Inputs.range([5, 50], {
      label: "Test cases",
      step: 1,
      value: 20
    }));
    const truePerformanceChange = view(Inputs.range([-0.2, 0.2], {
      label: "True change",
      step: 0.01,
      value: 0.05
    }));
    const noiseLevel = view(Inputs.range([0, 0.5], {
      label: "Noise level, σ",
      step: 0.01,
      value: 0.15
    }));
    const seed = 42;
  </script>
  <script id="75" type="module">
    // Make the controls sticky when scrolling
    const controlsCell = document.getElementById('cell-16');

    if (controlsCell) {
      // Store original position
      let scrollCheckEl = controlsCell;
      let isSticky = false;

      // Create a placeholder to preserve space when controls become fixed
      const placeholder = document.createElement('div');
      controlsCell.parentNode.insertBefore(placeholder, controlsCell);

      const handleScroll = () => {
        const scrollTop = window.pageYOffset || document.documentElement.scrollTop;

        const originalTop = scrollCheckEl.offsetTop;
        if (scrollTop > originalTop && !isSticky) {
          // Make sticky
          scrollCheckEl = placeholder;
          isSticky = true;
          const rect = controlsCell.getBoundingClientRect();
          placeholder.style.height = `${controlsCell.offsetHeight}px`;
          placeholder.style.display = 'block';
          controlsCell.style.position = 'fixed';
          controlsCell.style.top = '0';
          controlsCell.style.left = '0';
          controlsCell.style.right = '0';
          controlsCell.style.width = '100%';
          controlsCell.style.maxWidth = '640px';
          controlsCell.style.padding = '0 10px 10px 10px';
          controlsCell.style.margin = '0 auto';
          controlsCell.style.zIndex = '1000';
          controlsCell.style.backgroundColor = 'var(--theme-background, white)';
          controlsCell.style.boxShadow = '0 2px 8px rgba(0,0,0,0.1)';
        } else if (scrollTop <= originalTop && isSticky) {
          scrollCheckEl = controlsCell;
          // Remove sticky
          isSticky = false;
          placeholder.style.display = 'none';
          controlsCell.style.position = '';
          controlsCell.style.top = '';
          controlsCell.style.left = '';
          controlsCell.style.right = '';
          controlsCell.style.width = '';
          controlsCell.style.zIndex = '';
          controlsCell.style.margin = '';
          controlsCell.style.padding = '';
          controlsCell.style.backgroundColor = '';
          controlsCell.style.boxShadow = '';
        }
      };

      window.addEventListener('scroll', handleScroll, { passive: true });

      // Clean up on cell re-run
      invalidation.then(() => {
        window.removeEventListener('scroll', handleScroll);
        placeholder.remove();
        if (isSticky) {
          controlsCell.style.position = '';
          controlsCell.style.top = '';
          controlsCell.style.left = '';
          controlsCell.style.right = '';
          controlsCell.style.width = '';
          controlsCell.style.zIndex = '';
          controlsCell.style.backgroundColor = '';
          controlsCell.style.boxShadow = '';
        }
      });
    }
  </script>
  <script id="34" type="text/x-typescript">
    const baselineTimeMin = 10;
    const baselineTimeMax = 100;
    const testData = generatePerformanceData(
      numTestCases,
      baselineTimeMin,
      baselineTimeMax,
      noiseLevel,
      truePerformanceChange,
      seed
    );
    const testResult = wilcoxonSignedRank(testData);
    const chartData = testData.map(d => ({
      id: d.id,
      before: d.timeBefore,
      after: d.timeAfter,
      difference: d.timeAfter - d.timeBefore,
      percentChange: ((d.timeAfter - d.timeBefore) / d.timeBefore) * 100
    }));
  </script>
  <script id="60" type="text/markdown">
    ### Sample Data
  </script>
  <script id="35" type="text/markdown">
    Below is the synthetic data generated with the chosen parameters, showing before and after timings for each test case.
  </script>
  <script id="37" type="module">
    const dataTable = chartData.map(d => ({
      "Test Case": d.id,
      "Before (ms)": d.before.toFixed(2),
      "After (ms)": d.after.toFixed(2),
      "Difference (ms)": d.difference.toFixed(2)
    }));
    display(Inputs.table(dataTable, {
      width: "100%",
      maxHeight: 200
    }));
  </script>
  <script id="76" type="text/x-typescript">
    display(Plot.plot({
      height: 400,
      marginLeft: 60,
      marginBottom: 60,
      x: {label: "Time Before (ms)", grid: true},
      y: {label: "Time After (ms)", grid: true},
      marks: [
        // Diagonal line for no change
        Plot.line([[0, 0], [120, 120]], {stroke: "#ccc", strokeDasharray: "4 4"}),
        // Points
        Plot.dot(chartData, {
          x: "before",
          y: "after",
          fill: d => d.after > d.before ? "#e74c3c" : "#2ecc71",
          r: 5,
          title: d => `${d.id}\nBefore: ${d.before.toFixed(2)}ms\nAfter: ${d.after.toFixed(2)}ms\nChange: ${d.percentChange.toFixed(1)}%`
        }),
        // Connecting lines
        Plot.link(chartData, {
          x1: "before",
          y1: "before",
          x2: "before",
          y2: "after",
          stroke: d => d.after > d.before ? "#e74c3c" : "#2ecc71",
          strokeOpacity: 0.3,
          strokeWidth: 1
        })
      ],
      caption: "Each point shows before/after performance. Points above the diagonal line indicate performance degradation (slower)."
    }));
  </script>
  <script id="59" type="text/markdown">
    The table below shows these same differences ranked by magnitude. Notice we're ranking by **absolute time difference** rather than **percent change**. One or the other may be more appropriate, depending on how you expect performance to change across tests, for example whether by a fixed latency or a proportional change.
  </script>
  <script id="30" type="text/x-typescript">
    const rankedTable = testResult.ranks
      .sort((a, b) => a.rank - b.rank)
      .map(r => ({
        "Test Case": r.id,
        "Difference (ms)": r.diff.toFixed(3),
        "Absolute Difference": r.absDiff.toFixed(3),
        "Rank": r.rank,
        "Sign": r.diff > 0 ? "+" : "−"
      }));
  </script>
  <script id="31" type="module">
    display(Inputs.table(rankedTable, {
      width: "100%",
      maxHeight: 200
    }));
  </script>
  <script id="61" type="text/markdown">
    The test proceeds by computing the sum of the positive (W+) and negative (W-) ranks:

  </script>
  <script id="38" type="text/markdown">
    **Sample size (${tex`n`}):** ${testResult.n} paired observations<br>
    **Sum of positive ranks (${tex`W^+`}):** ${testResult.wPlus.toFixed(1)} (cases where algorithm got slower)<br>
    **Sum of negative ranks (${tex`W^−`}):** ${testResult.wMinus.toFixed(1)} (cases where algorithm got faster)<br>
    **Test statistic (${tex`W`}):** ${testResult.W.toFixed(1)}
  </script>
  <script id="88" type="text/markdown">
    ### Statistical Significance
  </script>
  <script id="62" type="text/markdown">
    With the test statistic W in hand, we compute a z-score and p-value using the normal approximation.
  </script>
  <script id="63" type="text/markdown">
    For reasonably large samples (${tex`n > 10`}), the test statistic ${tex`W`} is approximately normally distributed. We can compute the expected value and standard deviation under the null hypothesis (no systematic change):
  </script>
  <script id="64" type="text/x-typescript">
    const expectedW = testResult.n * (testResult.n + 1) / 4;
    const varianceW = testResult.n * (testResult.n + 1) * (2 * testResult.n + 1) / 24;
    const stdDevW = Math.sqrt(varianceW);
  </script>
  <script id="81" type="text/markdown">
    **Expected value:** ${tex`\mu_W = \frac{n(n+1)}{4} = \frac{${testResult.n} \cdot ${testResult.n + 1}}{4} = ${expectedW.toFixed(2)}`}
  </script>
  <script id="82" type="text/markdown">
    **Variance:** ${tex`\sigma^2_W = \frac{n(n+1)(2n+1)}{24} = \frac{${testResult.n} \cdot ${testResult.n + 1} \cdot ${2 * testResult.n + 1}}{24} = ${varianceW.toFixed(2)}`}
  </script>
  <script id="83" type="text/markdown">
    **Standard deviation:** ${tex`\sigma_W = \sqrt{\sigma^2_W} = ${stdDevW.toFixed(2)}`}
  </script>
  <script id="84" type="text/markdown">
    **Z-score (with continuity correction):**

    ${tex.block`
    \begin{aligned}
    z &= \frac{W - \mu_W + \frac{1}{2} \text{sgn}(W - \mu_W)}{\sigma_W} \\
      &= \frac{${testResult.W.toFixed(1)} - ${expectedW.toFixed(2)} + ${(0.5 * Math.sign(testResult.W - expectedW)).toFixed(1)}}{${stdDevW.toFixed(2)}} \\
      &= ${testResult.zScore.toFixed(3)}
    \end{aligned}`}
  </script>
  <script id="87" type="text/markdown">
    ### Test Results
  </script>
  <script id="66" type="text/markdown">
    ${testResult.wPlus > testResult.wMinus
      ? html`${tex`W^+ > W^-,`} so the algorithm appears to have <strong>degraded</strong>.`
      : html`${tex`W^+ \leq W^-,`} so the algorithm appears to have <strong>improved</strong>.`}

    **Z-score:** ${testResult.zScore.toFixed(3)}<br>
    **P-value:** ${testResult.pValue.toFixed(4)}<br>
    **Conclusion:** ${testResult.significant
      ? "p < 0.05, so the difference is **statistically significant**."
      : "p ≥ 0.05, so the difference is **not statistically significant**."}
  </script>
  <script id="89" type="text/markdown">
    ### Interpreting Results
  </script>
  <script id="67" type="text/markdown">
    The p-value tells us the probability of seeing differences this extreme if there were no performance change. A p-value below 0.05 provides strong evidence of a real performance change. A p-value of 0.05 or higher means insufficient evidence to conclude there's a systematic change.
  </script>
  <script id="91" type="text/markdown">
    ## Conclusion
  </script>
  <script id="93" type="text/markdown">
    At the end of the day, this was a lot more intuitive and less scary than I expected. We really just rank the changes and compare the rankings to what we’d expect if there were no performance difference. I like the precision and rigor with which it does this, and I like that it respects the relative size of changes without being fully sensitive to bizarre outliers. I’m still not great with statistics, but for what I need, it feels like the right tool for the job and I’m pretty content here.
  </script>
</notebook>
