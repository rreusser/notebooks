<!doctype html>
<notebook theme="air">
  <title>Strange Attractors in WebGPU</title>
  <script id="intro" type="text/markdown">
# Strange Attractors in WebGPU

*This notebook is a 2026 WebGPU update of my previous post, [Strange Attractors on the GPU, Part 1: Implementation](https://observablehq.com/@rreusser/strange-attractors-on-the-gpu-part-1).*

This notebook walks through simulating a strange attractor on the GPU and then rendering particle tracks as lines. The essential feature of this notebook is that it accomplishes both particle updates and rendering to the screen without data ever touching the CPU.
  </script>
  <script id="webgpu-setup" type="module">
// Check for WebGPU support
if (!navigator.gpu) {
  display(html`<p style="color: red;">WebGPU is not supported in this browser.</p>`);
  throw new Error('WebGPU not supported');
}

const adapter = await navigator.gpu.requestAdapter();
if (!adapter) {
  throw new Error('Failed to get WebGPU adapter');
}

const device = await adapter.requestDevice();
const canvasFormat = navigator.gpu.getPreferredCanvasFormat();

invalidation.then(() => device.destroy());
  </script>
  <script id="main-canvas" type="module">
import { createElementStack } from './lib/element-stack.js'
import { expandable } from './lib/expandable.js'

const dpr = window.devicePixelRatio || 1;
const canvasWidth = Math.min(800, width);
const canvasHeight = Math.max(400, canvasWidth * 0.6);

const stack = createElementStack({
  width: canvasWidth,
  height: canvasHeight,
  layers: [{
    id: 'canvas',
    element: ({ current, width, height }) => {
      const canvas = current || document.createElement('canvas');
      canvas.id = 'attractor-canvas';
      canvas.width = Math.floor(width * dpr);
      canvas.height = Math.floor(height * dpr);
      canvas.style.width = `${width}px`;
      canvas.style.height = `${height}px`;
      return canvas;
    }
  }, {
    id: 'svg',
    element: ({ current, width, height }) =>
      (current ? d3.select(current) : d3.create("svg"))
        .attr("width", width)
        .attr("height", height)
        .style("cursor", "grab")
        .node()
  }]
});

const canvas = stack.elements.canvas;

const gpuContext = canvas.getContext('webgpu');
gpuContext.configure({
  device,
  format: canvasFormat,
  alphaMode: 'premultiplied'
});

// Create depth texture for proper z-ordering
const depthTexture = device.createTexture({
  label: 'depth-texture',
  size: [canvas.width, canvas.height],
  format: 'depth24plus',
  usage: GPUTextureUsage.RENDER_ATTACHMENT
});

const renderState = { dirty: true, depthTexture };

const figure = html`<figure style="margin: 0;" id="main-figure">
  ${stack.element}
  <figcaption>Click <em>Restart</em> to reinitialize particles.</figcaption>
</figure>`;

display(expandable(figure, {
  width: canvasWidth,
  height: canvasHeight,
  controls: '.attractor-controls',
  onResize(el, w, h) {
    stack.resize(w, h);
    canvas.width = Math.floor(w * dpr);
    canvas.height = Math.floor(h * dpr);
    // Recreate depth texture at new size
    renderState.depthTexture.destroy();
    renderState.depthTexture = device.createTexture({
      label: 'depth-texture',
      size: [canvas.width, canvas.height],
      format: 'depth24plus',
      usage: GPUTextureUsage.RENDER_ATTACHMENT
    });
    renderState.dirty = true;
    stack.dispatchEvent(new CustomEvent('update'));
  }
}));
  </script>
  <script id="controls" type="module">
const controlsContainer = html`<div class="attractor-controls"></div>`;

function ctrl(input) {
  controlsContainer.appendChild(input);
  return Generators.input(input);
}

const restartInput = Inputs.button('Restart');
const restart = ctrl(restartInput);

const simulateInput = Inputs.toggle({ label: 'Simulate', value: true });
const simulate = ctrl(simulateInput);

const particleCountInput = Inputs.range([1, 4096], {
  value: 20,
  label: 'Particle count',
  step: 1
});
const particleCount = ctrl(particleCountInput);

const stepCountInput = Inputs.range([1, 1024], {
  label: 'Track length',
  value: 100,
  transform: Math.log,
  step: 1
});
const stepCount = ctrl(stepCountInput);

const dtInput = Inputs.range([0.001, 0.1], {
  value: 0.02,
  label: 'Time step'
});
const dt = ctrl(dtInput);

const lineWidthInput = Inputs.range([1, 20], {
  value: 5,
  label: 'Line width',
  step: 0.5
});
const lineWidth = ctrl(lineWidthInput);

display(controlsContainer);
  </script>
  <script id="explanation-attractor" type="text/markdown">
### The Attractor

A strange attractor is a set of states toward which a dynamical system evolves over time. The particular attractor we're simulating here is defined by the system of ordinary differential equations:

${tex.block`\begin{aligned}
\frac{dx}{dt} &= \alpha x(1 - y) - \beta z \\[0.5em]
\frac{dy}{dt} &= -\gamma y(1 - x^2) \\[0.5em]
\frac{dz}{dt} &= \mu x
\end{aligned}`}

with parameters ${tex`\alpha = 3`}, ${tex`\beta = 2.2`}, ${tex`\gamma = 1`}, ${tex`\mu = 1.51`}. These equations exhibit chaotic behavior; nearby trajectories diverge exponentially but remain bounded within the attractor's basin.
  </script>
  <script id="explanation-overview" type="text/markdown">
## From WebGL to WebGPU

[The original WebGL version of this simulation](https://observablehq.com/@rreusser/strange-attractors-on-the-gpu-part-1) stored particle state in a floating-point texture. Each row represented a particle's history as a ring buffer. The simulation was designed around the limitations of WebGL, in particular the lack of compute shaders and inability to read and write to the same texture (WebGL 2's [transform feedback](https://webgl2fundamentals.org/webgl/lessons/webgl-gpgpu.html#now-lets-do-it-with-transform-feedback) notwithstanding).

A naive WebGPU port would preserve this texture-based approach, replacing fragment shader hacks with proper compute shaders. That would require **three GPU operations per frame**:

1. **Integrate**: Read from the state texture, write new positions to a temporary texture
2. **Copy**: Transfer the temporary column back into the main state texture
3. **Draw**: Render lines by sampling the state texture in a vertex shader

WebGPU offers a better primitive for this workload which eliminates the copy step entirely: **storage buffers** with `read_write` access. This is an easy fit for the embarrassingly parallel task of integrating independent particles with no mutual interaction.

### The Optimized Approach

With storage buffers, each frame requires just **two GPU operations**:

1. **Integrate** the differential equation, reading the current state and writing the new state directly to the same buffer.
2. **Draw** all line segments, joins, and caps using instanced triangle strip geometry. Sentinel values separate the lines into multiple contiguous segments.

This approach reduces GPU dispatches by 33%, eliminates a temporary texture allocation, and simplifies the code substantially.
  </script>
  <script id="simulation-method" type="text/markdown">
## Simulation Method

To simulate strange attractors, we need to solve two challenges: stepping the differential equations and drawing lines.

For computation, we use WebGPU **compute shaders**. These allow us to dispatch parallel workloads that read from and write to GPU resources directly. Each particle's state update runs as an independent thread, making the simulation embarrassingly parallel.

For line rendering, we use the (WIP) [webgpu-lines](../webgpu-lines/) module. Lines are notoriously difficult to render well. The built-in line primitive in graphics APIs is typically limited to single-pixel width. To get smooth, variable-width lines with proper joins and caps, we build geometry from triangles in the vertex shader. The module handles this complexity, requiring only that we provide a function to compute vertex positions from our data.
  </script>
  <script id="explanation-state" type="text/markdown">
## State Layout

The state of our ordinary differential equation (ODE) is represented by the three-component vector ${tex`(x, y, z)`}. We store these in a flat storage buffer as `vec4f` elements. The ${tex`j^{th}`} time step of the ${tex`i^{th}`} particle is represented by the four-component vector:

${tex.block`\mathbf{p}_j^{(i)} = (x_j^{(i)}, y_j^{(i)}, z_j^{(i)}, 1)`}

### Why Storage Buffers?

The WebGL version used a 2D texture where each row was a particle and each column was a time step. Textures are a natural fit when you need 2D spatial locality or hardware filtering, but for our access pattern—where each particle only touches its own data—a flat buffer is simpler and more efficient.

Storage buffers offer several advantages over textures for this use case:

- **`read_write` access**: Unlike `rgba32float` textures, storage buffers can be both read and written in the same compute pass
- **Direct indexing**: Simple arithmetic replaces texel coordinate calculations with half-pixel offsets
- **No intermediate resources**: We eliminate the temporary texture needed for the copy step
- **Flexible layout**: We can choose any memory layout that suits our access pattern

We use **particle-major ordering**: all time steps for particle 0 come first, then all time steps for particle 1, and so on. The buffer index for a given particle and step is simply `particle * stepCount + step`.

### Ring Buffer

As we step the ODE, we compute one new history point for each particle track. To avoid having to shift the entire history on every iteration, we treat each particle's slice as a **ring buffer**. At each time step ${tex`j`}, we use the previous position, ${tex`p_{j-1}^{(i)}`}, to compute the next, ${tex`p_j^{(i)}`}. When we reach the end of the slice, we loop back to the start, overwriting the oldest time step with the newest.

This ring buffer approach means we only write one position per particle per frame, regardless of track length.
  </script>
  <script id="state-buffer" type="module">
particleCount; stepCount; restart;

// Create storage buffer for state
// Each element stores (x, y, z, 1) as vec4f for one particle at one timestep
// Layout: buffer[particleId * stepCount + stepIndex] = position
// Using storage buffer with read_write access eliminates the need for a temp buffer
const stateBuffer = device.createBuffer({
  label: 'state-buffer',
  size: particleCount * stepCount * 16, // vec4f (16 bytes) per position
  usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST
});

// Track current column for ring buffer
const simState = { currentColumn: 0, t: 0 };

invalidation.then(() => {
  stateBuffer.destroy();
});
  </script>
  <script id="explanation-init" type="text/markdown">
## Computation

Our computational primitive is the **compute shader**. Unlike fragment shaders which operate on pixels being rasterized, compute shaders are general-purpose: we dispatch a grid of threads and each thread can read from and write to arbitrary locations in GPU resources.

For our simulation, we dispatch one thread per particle. Each thread reads the current state and writes the updated state. This is a classic parallel map operation.

### Initialization

We start by initializing particle positions within a sphere centered near the attractor. Generating good pseudorandom numbers on a GPU is tricky, so we use a low-discrepancy **quasirandom** number generator described by Martin Roberts in *[The Unreasonable Effectiveness of Quasirandom Sequences](http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/)*.

The initialization shader writes the same starting position to every column of each particle's row—filling the entire ring buffer with the initial state.
  </script>
  <script id="init-shader" type="module">
// Compute shader to initialize particle state
const initShaderCode = /* wgsl */`
@group(0) @binding(0) var<storage, read_write> stateBuffer: array<vec4f>;

struct Uniforms {
  origin: vec3f,
  scale: f32,
  stepCount: u32,
  particleCount: u32,
}
@group(0) @binding(1) var<uniform> uniforms: Uniforms;

// Quasirandom sequence: http://extremelearning.com.au/unreasonable-effectiveness-of-quasirandom-sequences/
fn quasirandom(n: f32) -> vec3f {
  let g = 1.22074408460575947536;
  return fract(0.5 + n * vec3f(1.0 / g, 1.0 / (g * g), 1.0 / (g * g * g))).zyx;
}

fn sphericalRandom(n: f32) -> vec3f {
  let rand = quasirandom(n);
  let u = rand.x * 2.0 - 1.0;
  let theta = 6.283185307179586 * rand.y;
  let r = sqrt(1.0 - u * u);
  return vec3f(r * cos(theta), r * sin(theta), u) * sqrt(rand.z);
}

@compute @workgroup_size(64)
fn main(@builtin(global_invocation_id) gid: vec3u) {
  let particle = gid.x;
  if (particle >= uniforms.particleCount) { return; }

  // Initialize all columns for this particle with the same position
  let pos = uniforms.origin + uniforms.scale * sphericalRandom(f32(particle) + 0.5);
  let baseIdx = particle * uniforms.stepCount;
  for (var col = 0u; col < uniforms.stepCount; col++) {
    stateBuffer[baseIdx + col] = vec4f(pos, 1.0);
  }
}
`;

const initShaderModule = device.createShaderModule({
  label: 'init-shader',
  code: initShaderCode
});

const initPipeline = device.createComputePipeline({
  label: 'init-pipeline',
  layout: 'auto',
  compute: {
    module: initShaderModule,
    entryPoint: 'main'
  }
});

const initUniformBuffer = device.createBuffer({
  label: 'init-uniforms',
  size: 32, // vec3f origin (12) + f32 scale (4) + u32 stepCount (4) + u32 particleCount (4) + padding
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const initBindGroup = device.createBindGroup({
  label: 'init-bind-group',
  layout: initPipeline.getBindGroupLayout(0),
  entries: [
    { binding: 0, resource: { buffer: stateBuffer } },
    { binding: 1, resource: { buffer: initUniformBuffer } }
  ]
});

function initializeState() {
  // Update uniforms
  const uniformData = new ArrayBuffer(32);
  const f32 = new Float32Array(uniformData);
  const u32 = new Uint32Array(uniformData);
  f32[0] = 0;  // origin.x
  f32[1] = 1;  // origin.y (center near y=1 for this attractor)
  f32[2] = 0;  // origin.z
  f32[3] = 1;  // scale
  u32[4] = stepCount;
  u32[5] = particleCount;
  device.queue.writeBuffer(initUniformBuffer, 0, uniformData);

  const encoder = device.createCommandEncoder();
  const pass = encoder.beginComputePass();
  pass.setPipeline(initPipeline);
  pass.setBindGroup(0, initBindGroup);
  pass.dispatchWorkgroups(Math.ceil(particleCount / 64));
  pass.end();
  device.queue.submit([encoder.finish()]);

  simState.currentColumn = 0;
  simState.t = 0;
  renderState.dirty = true;
}

// Initialize on load and restart
initializeState();

invalidation.then(() => {
  initUniformBuffer.destroy();
});
  </script>
  <script id="explanation-integration" type="text/markdown">
### Integration

To step the ODE, we use the [fourth-order Runge-Kutta](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods#The_Runge%E2%80%93Kutta_method) (RK4) method. RK4 achieves high accuracy by sampling the derivative at four points within each time step and taking a weighted average.

With a texture-based approach, we'd need two compute passes: one to read from the state texture and write to a temporary texture (since `rgba32float` doesn't support simultaneous read/write), then another to copy that column back. That's two dispatches, two bind groups, and a temporary allocation—all for what should be a simple update.

Storage buffers let us collapse this into a **single compute pass**. The shader reads from index `particle * stepCount + srcColumn` and writes to `particle * stepCount + dstColumn`. Since these are different indices (no particle reads what another particle writes), there are no data races. One dispatch, one bind group, no temporary resources.
  </script>
  <script id="attractor-wgsl" type="module">
// The strange attractor equations
const attractorWGSL = /* wgsl */`
fn derivative(x: f32, y: f32, z: f32, t: f32) -> vec3f {
  let alpha = 3.0;
  let beta = 2.20;
  let gamma = 1.0;
  let mu = 1.510;
  return vec3f(
    alpha * x * (1.0 - y) - beta * z,
    -gamma * y * (1.0 - x * x),
    mu * x
  );
}
`;
  </script>
  <script id="integrate-shader" type="module">
// Integration shader: reads and writes to a single storage buffer
const integrateShaderCode = /* wgsl */`
@group(0) @binding(0) var<storage, read_write> stateBuffer: array<vec4f>;

struct Uniforms {
  dt: f32,
  t: f32,
  srcColumn: u32,
  dstColumn: u32,
  stepCount: u32,
  particleCount: u32,
}
@group(0) @binding(1) var<uniform> uniforms: Uniforms;

${attractorWGSL}

fn deriv(p: vec3f, t: f32) -> vec3f {
  return derivative(p.x, p.y, p.z, t);
}

@compute @workgroup_size(64)
fn main(@builtin(global_invocation_id) gid: vec3u) {
  let particle = gid.x;
  if (particle >= uniforms.particleCount) { return; }

  // Read current state from source column
  let readIdx = particle * uniforms.stepCount + uniforms.srcColumn;
  let p = stateBuffer[readIdx].xyz;

  // RK4 integration
  let dt = uniforms.dt;
  let t = uniforms.t;
  let k1 = deriv(p, t);
  let k2 = deriv(p + 0.5 * dt * k1, t + 0.5 * dt);
  let k3 = deriv(p + 0.5 * dt * k2, t + 0.5 * dt);
  let k4 = deriv(p + dt * k3, t + dt);

  var newP = p + (dt / 6.0) * (k1 + k4 + 2.0 * (k2 + k3));

  // If particle diverges, reset near origin
  if (dot(newP, newP) > 1e6) {
    newP = newP * 0.0001;
  }

  // Write to destination column
  let writeIdx = particle * uniforms.stepCount + uniforms.dstColumn;
  stateBuffer[writeIdx] = vec4f(newP, 1.0);
}
`;

const integrateShaderModule = device.createShaderModule({
  label: 'integrate-shader',
  code: integrateShaderCode
});

const integratePipeline = device.createComputePipeline({
  label: 'integrate-pipeline',
  layout: 'auto',
  compute: {
    module: integrateShaderModule,
    entryPoint: 'main'
  }
});

const integrateUniformBuffer = device.createBuffer({
  label: 'integrate-uniforms',
  size: 32, // f32 dt + f32 t + u32 srcColumn + u32 dstColumn + u32 stepCount + u32 particleCount + padding
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const integrateBindGroup = device.createBindGroup({
  label: 'integrate-bind-group',
  layout: integratePipeline.getBindGroupLayout(0),
  entries: [
    { binding: 0, resource: { buffer: stateBuffer } },
    { binding: 1, resource: { buffer: integrateUniformBuffer } }
  ]
});

invalidation.then(() => {
  integrateUniformBuffer.destroy();
});
  </script>
  <script id="explanation-rendering" type="text/markdown">
## Line Rendering

In the timeless words of Matt DesLauriers, *[Drawing Lines is Hard](https://mattdesl.svbtle.com/drawing-lines-is-hard)*. All browsers limit the built-in line primitive to a single pixel width, so for any reasonably well-rendered lines, we need to build our own geometry.

The [webgpu-lines](../webgpu-lines/) module handles this. It renders lines as instanced triangle strips, with one instance per line segment. Each instance draws the segment itself plus portions of the adjacent joins. A four-point sliding window (previous, start, end, next) provides the context needed to compute join geometry.

The key insight is that we don't pass vertex positions directly. Instead, we provide a **vertex function** that the module calls for each point index. This function can sample data from any source—in our case, the state buffer. The module handles all the complexity of building smooth joins and round caps.

### Buffer Index Lookup

Rather than copying positions to vertex buffers each frame, we sample them directly from the storage buffer in the vertex shader. Each vertex index maps to a buffer index:

${tex.block`\text{bufferIdx} = i \cdot M + ((j + \text{offset}) \mod M)`}

where ${tex`M`} is the number of time steps, ${tex`i`} is the particle index, and ${tex`j`} is the step within that particle's track.

To handle the ring buffer, we add an offset to the step index that shifts based on which column was most recently written. The oldest data is always at the "start" of the rendered line, and the newest at the end.

### Line Breaks

Multiple particle tracks are rendered in a single draw call. We separate them using **sentinel values**—vertices with `w = 0` in clip space. When the vertex function returns a position with `w = 0`, the line renderer interprets this as a break between separate lines.
  </script>
  <script id="camera-setup" type="module">
// Simple orbital camera using mouse drag
// phi = azimuthal angle (rotation around Y axis)
// theta = polar angle (elevation from horizontal plane, positive = looking down)
const cameraState = {
  phi: 0.8,
  theta: 0.3,
  distance: 7,
  center: [0, 1, 0],
  fov: Math.PI / 4,
  near: 0.01,
  far: 100
};

function getProjectionViewMatrix(aspectRatio) {
  const { phi, theta, distance, center, fov, near, far } = cameraState;

  // Camera position in spherical coordinates
  const x = center[0] + distance * Math.cos(theta) * Math.cos(phi);
  const y = center[1] + distance * Math.sin(theta);
  const z = center[2] + distance * Math.cos(theta) * Math.sin(phi);

  // View matrix (lookAt)
  const eye = [x, y, z];
  const forward = [center[0] - x, center[1] - y, center[2] - z];
  const len = Math.sqrt(forward[0]**2 + forward[1]**2 + forward[2]**2);
  forward[0] /= len; forward[1] /= len; forward[2] /= len;

  const up = [0, 1, 0];
  const right = [
    forward[1] * up[2] - forward[2] * up[1],
    forward[2] * up[0] - forward[0] * up[2],
    forward[0] * up[1] - forward[1] * up[0]
  ];
  const rlen = Math.sqrt(right[0]**2 + right[1]**2 + right[2]**2);
  right[0] /= rlen; right[1] /= rlen; right[2] /= rlen;

  const newUp = [
    right[1] * forward[2] - right[2] * forward[1],
    right[2] * forward[0] - right[0] * forward[2],
    right[0] * forward[1] - right[1] * forward[0]
  ];

  // View matrix
  const view = new Float32Array([
    right[0], newUp[0], -forward[0], 0,
    right[1], newUp[1], -forward[1], 0,
    right[2], newUp[2], -forward[2], 0,
    -(right[0]*eye[0] + right[1]*eye[1] + right[2]*eye[2]),
    -(newUp[0]*eye[0] + newUp[1]*eye[1] + newUp[2]*eye[2]),
    (forward[0]*eye[0] + forward[1]*eye[1] + forward[2]*eye[2]),
    1
  ]);

  // Projection matrix (perspective)
  const f = 1.0 / Math.tan(fov / 2);
  const rangeInv = 1 / (near - far);
  const proj = new Float32Array([
    f / aspectRatio, 0, 0, 0,
    0, f, 0, 0,
    0, 0, (near + far) * rangeInv, -1,
    0, 0, near * far * rangeInv * 2, 0
  ]);

  // Multiply proj * view
  const result = new Float32Array(16);
  for (let i = 0; i < 4; i++) {
    for (let j = 0; j < 4; j++) {
      let sum = 0;
      for (let k = 0; k < 4; k++) {
        sum += proj[i + k*4] * view[k + j*4];
      }
      result[i + j*4] = sum;
    }
  }

  return result;
}

// Wire up mouse drag for camera rotation
const svg = d3.select(stack.elements.svg);
let isDragging = false;
let lastX = 0, lastY = 0;

function onMouseMove(event) {
  if (!isDragging) return;
  const dx = event.clientX - lastX;
  const dy = event.clientY - lastY;
  cameraState.phi += dx * 0.01;
  cameraState.theta = Math.max(-Math.PI/2 + 0.01, Math.min(Math.PI/2 - 0.01, cameraState.theta + dy * 0.01));
  lastX = event.clientX;
  lastY = event.clientY;
  renderState.dirty = true;
}

function onMouseUp() {
  if (!isDragging) return;
  isDragging = false;
  svg.style('cursor', 'grab');
  window.removeEventListener('mousemove', onMouseMove);
  window.removeEventListener('mouseup', onMouseUp);
}

svg.on('mousedown', (event) => {
  event.preventDefault();  // Prevent text selection
  isDragging = true;
  lastX = event.clientX;
  lastY = event.clientY;
  svg.style('cursor', 'grabbing');
  // Attach to window so drag continues even when mouse leaves canvas
  window.addEventListener('mousemove', onMouseMove);
  window.addEventListener('mouseup', onMouseUp);
});

svg.on('wheel', (event) => {
  event.preventDefault();
  cameraState.distance *= 1 + event.deltaY * 0.001;
  cameraState.distance = Math.max(1, Math.min(50, cameraState.distance));
  renderState.dirty = true;
});
  </script>
  <script id="line-rendering" type="module">
import { createGPULines } from '../webgpu-lines/webgpu-lines.js';

// Vertex shader that samples the state buffer
const vertexShaderBody = /* wgsl */`
@group(1) @binding(0) var<storage, read> stateBuffer: array<vec4f>;
@group(1) @binding(1) var<uniform> projViewMatrix: mat4x4f;

struct LineUniforms {
  columnOffset: u32,
  stepCount: u32,
  particleCount: u32,
}
@group(1) @binding(2) var<uniform> lineUniforms: LineUniforms;

struct Vertex {
  position: vec4f,
  width: f32,
  particleId: f32,
}

fn getVertex(index: u32) -> Vertex {
  // Decode buffer index from vertex index
  // Index layout: for each particle, stepCount points, then a break
  let pointsPerParticle = lineUniforms.stepCount + 1u; // +1 for line break
  let particle = index / pointsPerParticle;
  let step = index % pointsPerParticle;

  // Check if this is a line break point
  if (step >= lineUniforms.stepCount) {
    return Vertex(vec4f(0.0, 0.0, 0.0, 0.0), 0.0, f32(particle));
  }

  // Compute buffer index with ring buffer offset
  let stepIndex = (step + lineUniforms.columnOffset) % lineUniforms.stepCount;
  let bufferIdx = particle * lineUniforms.stepCount + stepIndex;

  // Sample position from state buffer
  let pos = stateBuffer[bufferIdx].xyz;

  // Project to clip space
  let projected = projViewMatrix * vec4f(pos, 1.0);

  // Use particle ID for slight z offset to reduce z-fighting
  var p = projected;
  let w = p.w;
  p = p / w;
  p.z = p.z - 0.0002 * abs(f32(particle) / f32(lineUniforms.particleCount) - 0.5);
  p = p * w;

  return Vertex(p, uniforms.width, f32(particle) / f32(lineUniforms.particleCount));
}
`;

const fragmentShaderBody = /* wgsl */`
fn getColor(lineCoord: vec2f, particleId: f32) -> vec4f {
  let color1 = vec3f(0.55, 0.89, 0.65);
  let color2 = vec3f(0.11, 0.32, 0.65);
  let baseColor = mix(color1, color2, particleId);

  // 1-pixel border: lineCoord is in [-1, 1], so 1 pixel = 2/width in normalized coords
  let borderWidth = 2.0 / uniforms.width;
  let dist = length(lineCoord.xy);
  let borderColor = vec3f(1.0);
  let color = mix(baseColor, borderColor, smoothstep(1.0 - 2.0 * borderWidth, 1.0 - borderWidth, dist));

  return vec4f(color, 1.0);
}
`;

const gpuLines = createGPULines(device, {
  format: canvasFormat,
  depthFormat: 'depth24plus',
  join: 'bevel',
  cap: 'round',
  capResolution: 4,
  vertexShaderBody,
  fragmentShaderBody,
});

// Create buffers for line rendering
const projViewBuffer = device.createBuffer({
  label: 'proj-view-matrix',
  size: 64,
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const lineUniformBuffer = device.createBuffer({
  label: 'line-uniforms',
  size: 16, // columnOffset, stepCount, particleCount + padding
  usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST
});

const lineBindGroup = device.createBindGroup({
  layout: gpuLines.getBindGroupLayout(1),
  entries: [
    { binding: 0, resource: { buffer: stateBuffer } },
    { binding: 1, resource: { buffer: projViewBuffer } },
    { binding: 2, resource: { buffer: lineUniformBuffer } }
  ]
});

invalidation.then(() => {
  gpuLines.destroy();
  projViewBuffer.destroy();
  lineUniformBuffer.destroy();
});
  </script>
  <script id="axes-rendering" type="module">
// Axes renderer - X (red), Y (green), Z (blue)
// Uses createGPULines from the previous cell
const axisRange = 5;

// Axes vertex data: 3 axes, 2 points each, with line breaks between
// Layout: [X start, X end, break, Y start, Y end, break, Z start, Z end]
const axesPositions = new Float32Array([
  -axisRange, 0, 0, 1,  // X axis start
   axisRange, 0, 0, 1,  // X axis end
  0, 0, 0, 0,           // line break
  0, -axisRange, 0, 1,  // Y axis start
  0,  axisRange, 0, 1,  // Y axis end
  0, 0, 0, 0,           // line break
  0, 0, -axisRange, 1,  // Z axis start
  0, 0,  axisRange, 1,  // Z axis end
]);

// Color index: 0 = X (red), 1 = Y (green), 2 = Z (blue)
const axesColors = new Float32Array([
  0, 0, 0, 1, 1, 0, 2, 2
]);

const axesPositionBuffer = device.createBuffer({
  label: 'axes-positions',
  size: axesPositions.byteLength,
  usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST
});
device.queue.writeBuffer(axesPositionBuffer, 0, axesPositions);

const axesColorBuffer = device.createBuffer({
  label: 'axes-colors',
  size: axesColors.byteLength,
  usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST
});
device.queue.writeBuffer(axesColorBuffer, 0, axesColors);

const axesVertexShader = /* wgsl */`
@group(1) @binding(0) var<storage, read> positions: array<vec4f>;
@group(1) @binding(1) var<storage, read> colorIndices: array<f32>;
@group(1) @binding(2) var<uniform> projViewMatrix: mat4x4f;

struct Vertex {
  position: vec4f,
  width: f32,
  colorIndex: f32,
}

fn getVertex(index: u32) -> Vertex {
  let pos = positions[index];
  let colorIdx = colorIndices[index];
  let projected = projViewMatrix * vec4f(pos.xyz, 1.0);
  return Vertex(vec4f(projected.xyz, pos.w * projected.w), uniforms.width, colorIdx);
}
`;

const axesFragmentShader = /* wgsl */`
fn getColor(lineCoord: vec2f, colorIndex: f32) -> vec4f {
  // X = red, Y = green, Z = blue
  var color: vec3f;
  let idx = i32(colorIndex + 0.5);
  if (idx == 0) {
    color = vec3f(0.9, 0.2, 0.2);  // Red for X
  } else if (idx == 1) {
    color = vec3f(0.2, 0.8, 0.2);  // Green for Y
  } else {
    color = vec3f(0.2, 0.4, 0.9);  // Blue for Z
  }
  return vec4f(color, 1.0);
}
`;

const axesLines = createGPULines(device, {
  format: canvasFormat,
  depthFormat: 'depth24plus',
  join: 'miter',
  cap: 'round',
  capResolution: 4,
  vertexShaderBody: axesVertexShader,
  fragmentShaderBody: axesFragmentShader,
});

const axesBindGroup = device.createBindGroup({
  layout: axesLines.getBindGroupLayout(1),
  entries: [
    { binding: 0, resource: { buffer: axesPositionBuffer } },
    { binding: 1, resource: { buffer: axesColorBuffer } },
    { binding: 2, resource: { buffer: projViewBuffer } }  // Reuse projViewBuffer
  ]
});

invalidation.then(() => {
  axesLines.destroy();
  axesPositionBuffer.destroy();
  axesColorBuffer.destroy();
});
  </script>
  <script id="explanation-draw-loop" type="text/markdown">
## Draw Loop

Finally, we put it all together into a frame loop. Each frame:

1. If simulating, dispatch the **integrate** compute shader to compute new particle positions
2. Update the camera's projection-view matrix
3. Update the buffer offset uniform so lines render from oldest to newest
4. Begin a render pass and **draw lines** by calling into the webgpu-lines module

The simulation and rendering are decoupled—we can pause simulation while still allowing camera interaction. The `renderState.dirty` flag tracks whether we need to redraw.
  </script>
  <script id="render-loop" type="module">
import { createFrameLoop } from './lib/frame-loop.js';

lineWidth; simulate; dt;

const loop = createFrameLoop(() => {
  // Simulation step
  if (simulate) {
    // Calculate source and destination columns
    const srcColumn = simState.currentColumn;
    const dstColumn = (simState.currentColumn + 1) % stepCount;

    // Update integration uniforms (dt, t, srcColumn, dstColumn, stepCount, particleCount)
    const integrateData = new ArrayBuffer(32);
    const f32 = new Float32Array(integrateData);
    const u32 = new Uint32Array(integrateData);
    f32[0] = dt;
    f32[1] = simState.t;
    u32[2] = srcColumn;
    u32[3] = dstColumn;
    u32[4] = stepCount;
    u32[5] = particleCount;
    device.queue.writeBuffer(integrateUniformBuffer, 0, integrateData);

    // Run integration (single pass reads and writes to storage buffer)
    const encoder = device.createCommandEncoder();
    const integratePass = encoder.beginComputePass();
    integratePass.setPipeline(integratePipeline);
    integratePass.setBindGroup(0, integrateBindGroup);
    integratePass.dispatchWorkgroups(Math.ceil(particleCount / 64));
    integratePass.end();
    device.queue.submit([encoder.finish()]);

    // Advance time and column
    simState.t += dt;
    simState.currentColumn = dstColumn;
    renderState.dirty = true;
  }

  // Render
  if (renderState.dirty) {
    const aspectRatio = canvas.width / canvas.height;
    const projView = getProjectionViewMatrix(aspectRatio);
    device.queue.writeBuffer(projViewBuffer, 0, projView);

    // Update line uniforms (columnOffset, stepCount, particleCount as u32)
    const lineData = new ArrayBuffer(16);
    const u32 = new Uint32Array(lineData);
    // columnOffset: the oldest column, so lines render from oldest to newest
    u32[0] = (simState.currentColumn + 1) % stepCount;
    u32[1] = stepCount;
    u32[2] = particleCount;
    device.queue.writeBuffer(lineUniformBuffer, 0, lineData);

    const encoder = device.createCommandEncoder();

    const pass = encoder.beginRenderPass({
      colorAttachments: [{
        view: gpuContext.getCurrentTexture().createView(),
        loadOp: 'clear',
        storeOp: 'store',
        clearValue: { r: 1.0, g: 1.0, b: 1.0, a: 1.0 }
      }],
      depthStencilAttachment: {
        view: renderState.depthTexture.createView(),
        depthClearValue: 1.0,
        depthLoadOp: 'clear',
        depthStoreOp: 'store'
      }
    });

    // Draw axes first (so they appear behind the attractor)
    // axesLines.draw(pass, {
    //   vertexCount: 8,  // 3 axes × 2 points + 2 line breaks
    //   width: 2 * dpr,
    //   resolution: [canvas.width, canvas.height]
    // }, [axesBindGroup]);

    // Total vertex count: (stepCount + 1) per particle for line breaks
    const totalVertexCount = particleCount * (stepCount + 1);

    gpuLines.draw(pass, {
      vertexCount: totalVertexCount,
      width: lineWidth * dpr,
      resolution: [canvas.width, canvas.height]
    }, [lineBindGroup]);

    pass.end();
    device.queue.submit([encoder.finish()]);

    if (!simulate) {
      renderState.dirty = false;
    }
  }
});

invalidation.then(() => loop.cancel());
  </script>
  <script id="restart-handler" type="module">
// Handle restart button
restart;
initializeState();
  </script>
  <script id="conclusion" type="text/markdown">
## Summary

We've built a complete GPU-based particle simulation with the following components:

- **State storage**: A flat storage buffer with particle-major ordering, where each particle's history is a ring buffer
- **Initialization**: A compute shader that fills the state with quasirandom starting positions
- **Integration**: A compute shader that advances the ODE using RK4, reading and writing directly to the same storage buffer
- **Rendering**: Instanced line drawing that samples positions directly from the state buffer

### Optimization Gains

By using WebGPU storage buffers instead of naively porting the WebGL texture-based approach, we achieved:

| Aspect | Texture Approach | Storage Buffer | Improvement |
|--------|------------------|----------------|-------------|
| GPU operations/frame | 3 (integrate + copy + render) | 2 (integrate + render) | 33% fewer dispatches |
| Compute bind groups | 2 (integrate + copy) | 1 (integrate only) | 50% reduction |
| Temporary resources | 1 column texture | None | Eliminated |
| Index calculation | Texel center math | Direct array index | Simpler code |

The entire simulation runs on the GPU. The CPU only dispatches work and updates a handful of uniform values each frame. This architecture scales well—you can simulate thousands of particles with long track histories while maintaining interactive frame rates.

### Generalizing

The techniques here generalize beyond strange attractors. Any particle system where you want to visualize trails—fluid simulations, n-body systems, gradient flows—can use this same ring buffer approach for efficient GPU-based track rendering. The key insight is recognizing when your access pattern allows `read_write` storage buffers: if each thread reads and writes to non-overlapping locations, you can collapse multi-pass texture ping-ponging into a single dispatch.
  </script>
</notebook>
